{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocess all the training data.\n",
    "Builds a vocabulary and converts all sentences into numerical indices\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = os.environ.get(\"P4_DATA_ROOT\", \"data/\")\n",
    "\n",
    "\n",
    "def read_labeled_sentences(f):\n",
    "    out = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, sent = line.split(\"\\t\")\n",
    "        words = sent.split()\n",
    "        bool_label = label == 'POS'\n",
    "        out.append((bool_label, words))\n",
    "    return out\n",
    "\n",
    "\n",
    "def read_unlabeled_sentences(f):\n",
    "    out = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        out.append(words)\n",
    "    return out\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, \"train.txt\")) as f:\n",
    "        train_sents = read_labeled_sentences(f)\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, \"valid.txt\")) as f:\n",
    "        valid_sents = read_labeled_sentences(f)\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, \"test.txt\")) as f:\n",
    "        test_sents = read_labeled_sentences(f)\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, \"unlabeled.txt\")) as f:\n",
    "        unlabeled_sents = read_unlabeled_sentences(f)\n",
    "\n",
    "\n",
    "    # establish the vocabulary\n",
    "    ##########################\n",
    "\n",
    "    # start with list of most common words, labeled or unlabeled\n",
    "    word_counter = Counter(w for _, sent in train_sents for w in sent)\n",
    "    word_counter.update(w for sent in unlabeled_sents for w in sent)\n",
    "\n",
    "    # toss words that don't appear at least 10 times\n",
    "    vocab = [w for w, _ in word_counter.most_common()\n",
    "             if word_counter[w] >= 10]\n",
    "\n",
    "    # add special tokens\n",
    "    unk = '__UNK__'\n",
    "    s_start = '__START__'\n",
    "    s_end = '__END__'\n",
    "    vocab = [unk, s_start, s_end] + vocab\n",
    "\n",
    "    # build inverse vocabulary\n",
    "    # i.e. if vocab[17] = 'cat', then inv_vocab['cat'] = 17\n",
    "    inv_vocab = {w: k for k, w in enumerate(vocab)}\n",
    "\n",
    "    def sentence_to_ix(sent):\n",
    "        sent = [s_start] + sent + [s_end]\n",
    "        sent_ix = [inv_vocab.get(w, inv_vocab[unk])\n",
    "                   for w in sent]\n",
    "        return sent_ix\n",
    "\n",
    "    # vectorize and save sentence as lists of ids\n",
    "    #############################################\n",
    "\n",
    "    train_ix = [(y, sentence_to_ix(sent)) for y, sent in train_sents]\n",
    "    valid_ix = [(y, sentence_to_ix(sent)) for y, sent in valid_sents]\n",
    "    test_ix = [(y, sentence_to_ix(sent)) for y, sent in test_sents]\n",
    "    unlab_ix = [(None, sentence_to_ix(sent)) for sent in unlabeled_sents]\n",
    "\n",
    "    if not os.path.exists(\"processed\"):\n",
    "        os.makedirs(\"processed\")\n",
    "\n",
    "    with open(os.path.join(\"processed\", \"vocab.txt\"), \"w\") as f:\n",
    "        for w in vocab:\n",
    "            print(w, file=f)\n",
    "\n",
    "    with open(os.path.join(\"processed\", \"train_ix.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(train_ix, f)\n",
    "\n",
    "    with open(os.path.join(\"processed\", \"unlab_ix.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(unlab_ix, f)\n",
    "\n",
    "    with open(os.path.join(\"processed\", \"valid_ix.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(valid_ix, f)\n",
    "\n",
    "    with open(os.path.join(\"processed\", \"test_ix.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(test_ix, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4748\n"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 1.3s. Train loss:  0.68095 Valid accuracy:    58.31\n",
      "Epoch   1 took 1.7s. Train loss:  0.64478 Valid accuracy:    59.21\n",
      "Epoch   2 took 1.9s. Train loss:  0.60173 Valid accuracy:    59.64\n",
      "Epoch   3 took 1.4s. Train loss:  0.56313 Valid accuracy:    59.74\n",
      "Epoch   4 took 1.4s. Train loss:  0.53092 Valid accuracy:    59.69\n",
      "Epoch   5 took 1.4s. Train loss:  0.50447 Valid accuracy:    59.69\n",
      "Epoch   6 took 1.3s. Train loss:  0.48258 Valid accuracy:    59.57\n",
      "Epoch   7 took 1.4s. Train loss:  0.46429 Valid accuracy:    59.49\n",
      "Epoch   8 took 1.3s. Train loss:  0.44882 Valid accuracy:    59.22\n",
      "Epoch   9 took 1.5s. Train loss:  0.43569 Valid accuracy:    58.79\n",
      "Epoch  10 took 1.4s. Train loss:  0.42457 Valid accuracy:    58.59\n",
      "Epoch  11 took 1.3s. Train loss:  0.41507 Valid accuracy:    58.45\n",
      "Epoch  12 took 1.4s. Train loss:  0.40677 Valid accuracy:    58.26\n",
      "Epoch  13 took 1.3s. Train loss:  0.39937 Valid accuracy:    58.17\n",
      "Epoch  14 took 1.5s. Train loss:  0.39268 Valid accuracy:    58.08\n",
      "Epoch  15 took 1.3s. Train loss:  0.38661 Valid accuracy:    57.95\n",
      "Epoch  16 took 1.2s. Train loss:  0.38109 Valid accuracy:    57.87\n",
      "Epoch  17 took 1.3s. Train loss:  0.37600 Valid accuracy:    57.73\n",
      "Epoch  18 took 1.3s. Train loss:  0.37128 Valid accuracy:    57.62\n",
      "Epoch  19 took 1.2s. Train loss:  0.36692 Valid accuracy:    57.45\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Simple deep averaging net classifier\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class DANClassifier(object):\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "\n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.w_clf = params.add_parameters((1, hidden_dim))\n",
    "        self.b_clf = params.add_parameters((1))\n",
    "\n",
    "    def _predict(self, batch, train=True):\n",
    "\n",
    "        # load the network parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "        w_clf = dy.parameter(self.w_clf)\n",
    "        b_clf = dy.parameter(self.b_clf)\n",
    "\n",
    "        probas = []\n",
    "        # predict the probability of positive sentiment for each sentence\n",
    "        for _, sent in batch:\n",
    "\n",
    "            sent_embed = [dy.lookup(self.embed, w) for w in sent]\n",
    "            sent_embed = dy.average(sent_embed)\n",
    "\n",
    "            # hid = tanh(b + W * sent_embed)\n",
    "            # but it's faster to use affine_transform in dynet\n",
    "            hid = dy.affine_transform([b_hid, W_hid, sent_embed])\n",
    "            hid = dy.tanh(hid)\n",
    "\n",
    "            y_score = dy.affine_transform([b_clf, w_clf, hid])\n",
    "            y_proba = dy.logistic(y_score)\n",
    "            probas.append(y_proba)\n",
    "\n",
    "        return probas\n",
    "\n",
    "    def batch_loss(self, sents, train=True):\n",
    "        probas = self._predict(sents, train)\n",
    "\n",
    "        # we pack all predicted probas into one vector of length batch_size\n",
    "        probas = dy.concatenate(probas)\n",
    "\n",
    "        # we make a dynet vector out of the true ys\n",
    "        y_true = dy.inputVector([y for y, _ in sents])\n",
    "\n",
    "        # classification loss: we use the logistic loss\n",
    "        # this function automatically sums over all entries.\n",
    "        total_loss = dy.binary_log_loss(probas, y_true)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def num_correct(self, sents):\n",
    "        probas = self._predict(sents, train=False)\n",
    "        probas = [p.value() for p in probas]\n",
    "        y_true = [y for y, _ in sents]\n",
    "\n",
    "        correct = 0\n",
    "        # FIXME: count the number of correct predictions here\n",
    "        # Task 2\n",
    "        y_pred = []\n",
    "        for p in probas:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "\n",
    "        correct = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_true[i]])\n",
    "\n",
    "        return correct\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    clf = DANClassifier(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    train_batches = make_batches(train_ix, BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, BATCH_SIZE)\n",
    "\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = clf.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate # correct pred.\n",
    "        valid_acc = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            valid_acc += clf.num_correct(batch)\n",
    "\n",
    "        valid_acc /= len(valid_ix)\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train loss: {:8.5f} \"\n",
    "               \"Valid accuracy: {:8.2f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            total_loss / len(train_ix),\n",
    "            valid_acc * 100\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 2.1s. Train loss:  0.69038 Valid accuracy:    56.81\n",
      "Epoch   1 took 2.1s. Train loss:  0.66849 Valid accuracy:    58.48\n",
      "Epoch   2 took 2.2s. Train loss:  0.64671 Valid accuracy:    59.67\n",
      "Epoch   3 took 2.1s. Train loss:  0.62248 Valid accuracy:    60.22\n",
      "Epoch   4 took 2.1s. Train loss:  0.60132 Valid accuracy:    60.17\n",
      "Epoch   5 took 2.1s. Train loss:  0.58470 Valid accuracy:    59.87\n",
      "Epoch   6 took 1.9s. Train loss:  0.56936 Valid accuracy:    60.00\n",
      "Epoch   7 took 2.1s. Train loss:  0.55259 Valid accuracy:    60.61\n",
      "Epoch   8 took 2.1s. Train loss:  0.54465 Valid accuracy:    60.65\n",
      "Epoch   9 took 2.0s. Train loss:  0.52997 Valid accuracy:    60.72\n",
      "Epoch  10 took 2.3s. Train loss:  0.51847 Valid accuracy:    60.41\n",
      "Epoch  11 took 2.3s. Train loss:  0.51202 Valid accuracy:    60.66\n",
      "Epoch  12 took 2.2s. Train loss:  0.49826 Valid accuracy:    60.22\n",
      "Epoch  13 took 1.9s. Train loss:  0.49302 Valid accuracy:    60.83\n",
      "Epoch  14 took 1.8s. Train loss:  0.49155 Valid accuracy:    60.34\n",
      "Epoch  15 took 1.7s. Train loss:  0.48218 Valid accuracy:    60.36\n",
      "Epoch  16 took 1.8s. Train loss:  0.47483 Valid accuracy:    60.47\n",
      "Epoch  17 took 2.0s. Train loss:  0.47133 Valid accuracy:    60.52\n",
      "Epoch  18 took 2.2s. Train loss:  0.46630 Valid accuracy:    60.63\n",
      "Epoch  19 took 2.3s. Train loss:  0.45728 Valid accuracy:    60.35\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Simple deep averaging net classifier\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class DANClassifier(object):\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "\n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.w_clf = params.add_parameters((1, hidden_dim))\n",
    "        self.b_clf = params.add_parameters((1))\n",
    "\n",
    "    def _predict(self, batch, train=True):\n",
    "\n",
    "        # load the network parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "        w_clf = dy.parameter(self.w_clf)\n",
    "        b_clf = dy.parameter(self.b_clf)\n",
    "\n",
    "        probas = []\n",
    "        # predict the probability of positive sentiment for each sentence\n",
    "        for _, sent in batch:\n",
    "            sent_embed = [dy.lookup(self.embed, w) for w in sent]\n",
    "            # Task 3\n",
    "            if train == True:\n",
    "                for i in range(len(sent_embed)):\n",
    "                    sent_embed[i] = dy.dropout(sent_embed[i], 0.5)\n",
    "\n",
    "            sent_embed = dy.average(sent_embed)\n",
    "\n",
    "            # hid = tanh(b + W * sent_embed)\n",
    "            # but it's faster to use affine_transform in dynet\n",
    "            hid = dy.affine_transform([b_hid, W_hid, sent_embed])\n",
    "            hid = dy.tanh(hid)\n",
    "\n",
    "            y_score = dy.affine_transform([b_clf, w_clf, hid])\n",
    "            y_proba = dy.logistic(y_score)\n",
    "            probas.append(y_proba)\n",
    "\n",
    "        return probas\n",
    "\n",
    "    def batch_loss(self, sents, train=True):\n",
    "        probas = self._predict(sents, train)\n",
    "\n",
    "        # we pack all predicted probas into one vector of length batch_size\n",
    "        probas = dy.concatenate(probas)\n",
    "\n",
    "        # we make a dynet vector out of the true ys\n",
    "        y_true = dy.inputVector([y for y, _ in sents])\n",
    "\n",
    "        # classification loss: we use the logistic loss\n",
    "        # this function automatically sums over all entries.\n",
    "        total_loss = dy.binary_log_loss(probas, y_true)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def num_correct(self, sents):\n",
    "        probas = self._predict(sents, train=False)\n",
    "        probas = [p.value() for p in probas]\n",
    "        y_true = [y for y, _ in sents]\n",
    "\n",
    "        correct = 0\n",
    "        # FIXME: count the number of correct predictions here\n",
    "        # Task 2\n",
    "        y_pred = []\n",
    "        for p in probas:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "\n",
    "        correct = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_true[i]])\n",
    "\n",
    "        return correct\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    clf = DANClassifier(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    train_batches = make_batches(train_ix, BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, BATCH_SIZE)\n",
    "\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = clf.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate # correct pred.\n",
    "        valid_acc = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            valid_acc += clf.num_correct(batch)\n",
    "\n",
    "        valid_acc /= len(valid_ix)\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train loss: {:8.5f} \"\n",
    "               \"Valid accuracy: {:8.2f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            total_loss / len(train_ix),\n",
    "            valid_acc * 100\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 25.8s. Train perplexity:  239.070 Valid perplexity:  140.395\n",
      "Epoch   1 took 26.0s. Train perplexity:  137.390 Valid perplexity:  121.028\n",
      "Epoch   2 took 26.3s. Train perplexity:  120.594 Valid perplexity:  112.584\n",
      "Epoch   3 took 26.2s. Train perplexity:  111.206 Valid perplexity:  107.999\n",
      "Epoch   4 took 25.6s. Train perplexity:  105.055 Valid perplexity:  105.082\n",
      "Epoch   5 took 26.3s. Train perplexity:  100.566 Valid perplexity:  103.089\n",
      "Epoch   6 took 24.5s. Train perplexity:   97.064 Valid perplexity:  101.645\n",
      "Epoch   7 took 26.4s. Train perplexity:   94.201 Valid perplexity:  100.564\n",
      "Epoch   8 took 24.9s. Train perplexity:   91.791 Valid perplexity:   99.740\n",
      "Epoch   9 took 25.2s. Train perplexity:   89.724 Valid perplexity:   99.098\n",
      "Epoch  10 took 25.7s. Train perplexity:   87.926 Valid perplexity:   98.596\n",
      "Epoch  11 took 26.7s. Train perplexity:   86.341 Valid perplexity:   98.203\n",
      "Epoch  12 took 25.4s. Train perplexity:   84.926 Valid perplexity:   97.898\n",
      "Epoch  13 took 25.6s. Train perplexity:   83.661 Valid perplexity:   97.661\n",
      "Epoch  14 took 26.2s. Train perplexity:   82.517 Valid perplexity:   97.477\n",
      "Epoch  15 took 26.3s. Train perplexity:   81.475 Valid perplexity:   97.338\n",
      "Epoch  16 took 24.9s. Train perplexity:   80.524 Valid perplexity:   97.239\n",
      "Epoch  17 took 26.2s. Train perplexity:   79.653 Valid perplexity:   97.171\n",
      "Epoch  18 took 26.3s. Train perplexity:   78.852 Valid perplexity:   97.126\n",
      "Epoch  19 took 26.1s. Train perplexity:   78.111 Valid perplexity:   97.099\n",
      "Saving embeddings to embeds_baseline_lm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Simplest possible neural language model:\n",
    "    use word w_i to predict word w_(i + 1)\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "from math import exp\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "USE_UNLABELED = False\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "valid_perplexity_labeled_bigram = []\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class SimpleNLM(object):\n",
    "\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "\n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.W_out = params.add_parameters((vocab_size, hidden_dim))\n",
    "\n",
    "    def batch_loss(self, batch, train=True):\n",
    "\n",
    "        # load the parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "\n",
    "        W_out = dy.parameter(self.W_out)\n",
    "\n",
    "        losses = []\n",
    "        for _, sent in batch:\n",
    "            for i in range(1, len(sent)):\n",
    "                prev_word_ix = sent[i - 1]\n",
    "                curr_word_ix = sent[i]\n",
    "\n",
    "                ctx = dy.lookup(self.embed, prev_word_ix)\n",
    "\n",
    "                # hid is the hidden layer output, size=hidden_size\n",
    "                # compute b_hid + W_hid * ctx, but faster\n",
    "                hid = dy.affine_transform([b_hid, W_hid, ctx])\n",
    "                hid = dy.tanh(hid)\n",
    "\n",
    "                # out is the prediction of the next word, size=vocab_size\n",
    "                out = W_out * hid\n",
    "\n",
    "                # Intepretation: The model estimates that\n",
    "                # log P(curr_word=k | prev_word) ~ out[k]\n",
    "                # in other words,\n",
    "                # P(curr_word=k | prev_word) = exp(out[k]) / sum_j exp(out[j])\n",
    "                #                            = softmax(out)[k]\n",
    "\n",
    "                # We want to maximize the probability of the correct word.\n",
    "                # (equivalently, minimize the negative log-probability)\n",
    "\n",
    "                loss = dy.pickneglogsoftmax(out, curr_word_ix)\n",
    "                losses.append(loss)\n",
    "\n",
    "        # esum simply adds up the expressions in the list\n",
    "        return dy.esum(losses)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    if USE_UNLABELED:\n",
    "        __FIXME__\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    lm = SimpleNLM(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    train_batches = make_batches(train_ix, batch_size=BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, batch_size=BATCH_SIZE)\n",
    "\n",
    "    n_train_words = sum(len(sent) for _, sent in train_ix)\n",
    "    n_valid_words = sum(len(sent) for _, sent in valid_ix)\n",
    "\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate loss.\n",
    "        valid_loss = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=False)\n",
    "            valid_loss += loss.value()\n",
    "\n",
    "        toc = clock()\n",
    "        \n",
    "        # Task 4\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train perplexity: {:8.3f} \"\n",
    "               \"Valid perplexity: {:8.3f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            exp(total_loss / n_train_words),\n",
    "            exp(valid_loss / n_valid_words)\n",
    "            ))\n",
    "        valid_perplexity_labeled_bigram.append(exp(valid_loss / n_valid_words))\n",
    "        \n",
    "    # FIXME: make sure to update filenames when implementing ngram models\n",
    "    fn = \"embeds_baseline_lm\"\n",
    "    if USE_UNLABELED:\n",
    "        fn += \"_unlabeled\"\n",
    "\n",
    "    print(\"Saving embeddings to {}\".format(fn))\n",
    "    lm.embed.save(fn, \"/embed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 45.1s. Train perplexity:  167.268 Valid perplexity:  114.813\n",
      "Epoch   1 took 45.4s. Train perplexity:  114.421 Valid perplexity:  103.108\n",
      "Epoch   2 took 45.8s. Train perplexity:  104.174 Valid perplexity:   98.189\n",
      "Epoch   3 took 44.7s. Train perplexity:   98.539 Valid perplexity:   95.357\n",
      "Epoch   4 took 47.2s. Train perplexity:   94.822 Valid perplexity:   93.558\n",
      "Epoch   5 took 43.0s. Train perplexity:   92.155 Valid perplexity:   92.301\n",
      "Epoch   6 took 44.6s. Train perplexity:   90.113 Valid perplexity:   91.373\n",
      "Epoch   7 took 42.5s. Train perplexity:   88.486 Valid perplexity:   90.674\n",
      "Epoch   8 took 45.6s. Train perplexity:   87.146 Valid perplexity:   90.133\n",
      "Epoch   9 took 45.4s. Train perplexity:   86.009 Valid perplexity:   89.704\n",
      "Epoch  10 took 45.2s. Train perplexity:   85.034 Valid perplexity:   89.362\n",
      "Epoch  11 took 43.6s. Train perplexity:   84.192 Valid perplexity:   89.088\n",
      "Epoch  12 took 44.5s. Train perplexity:   83.455 Valid perplexity:   88.872\n",
      "Epoch  13 took 43.3s. Train perplexity:   82.806 Valid perplexity:   88.695\n",
      "Epoch  14 took 44.2s. Train perplexity:   82.226 Valid perplexity:   88.545\n",
      "Epoch  15 took 44.7s. Train perplexity:   81.708 Valid perplexity:   88.420\n",
      "Epoch  16 took 43.3s. Train perplexity:   81.240 Valid perplexity:   88.316\n",
      "Epoch  17 took 44.7s. Train perplexity:   80.816 Valid perplexity:   88.226\n",
      "Epoch  18 took 44.9s. Train perplexity:   80.429 Valid perplexity:   88.146\n",
      "Epoch  19 took 45.2s. Train perplexity:   80.075 Valid perplexity:   88.072\n",
      "Saving embeddings to embeds_baseline_lm_unlabeled\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Simplest possible neural language model:\n",
    "    use word w_i to predict word w_(i + 1)\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "from math import exp\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "USE_UNLABELED = True\n",
    "VOCAB_SIZE = len(vocab) #__FIXME__\n",
    "valid_perplexity_all_bigram = []\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class SimpleNLM(object):\n",
    "\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "\n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.W_out = params.add_parameters((vocab_size, hidden_dim))\n",
    "\n",
    "    def batch_loss(self, batch, train=True):\n",
    "\n",
    "        # load the parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "\n",
    "        W_out = dy.parameter(self.W_out)\n",
    "\n",
    "        losses = []\n",
    "        for _, sent in batch:\n",
    "            for i in range(1, len(sent)):\n",
    "                prev_word_ix = sent[i - 1]\n",
    "                curr_word_ix = sent[i]\n",
    "\n",
    "                ctx = dy.lookup(self.embed, prev_word_ix)\n",
    "\n",
    "                # hid is the hidden layer output, size=hidden_size\n",
    "                # compute b_hid + W_hid * ctx, but faster\n",
    "                hid = dy.affine_transform([b_hid, W_hid, ctx])\n",
    "                hid = dy.tanh(hid)\n",
    "\n",
    "                # out is the prediction of the next word, size=vocab_size\n",
    "                out = W_out * hid\n",
    "\n",
    "                # Intepretation: The model estimates that\n",
    "                # log P(curr_word=k | prev_word) ~ out[k]\n",
    "                # in other words,\n",
    "                # P(curr_word=k | prev_word) = exp(out[k]) / sum_j exp(out[j])\n",
    "                #                            = softmax(out)[k]\n",
    "\n",
    "                # We want to maximize the probability of the correct word.\n",
    "                # (equivalently, minimize the negative log-probability)\n",
    "\n",
    "                loss = dy.pickneglogsoftmax(out, curr_word_ix)\n",
    "                losses.append(loss)\n",
    "\n",
    "        # esum simply adds up the expressions in the list\n",
    "        return dy.esum(losses)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "    if USE_UNLABELED:\n",
    "        #__FIXME__\n",
    "        with open(os.path.join('processed', 'unlab_ix.pkl'), 'rb') as f:\n",
    "            train_ix += pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    lm = SimpleNLM(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    train_batches = make_batches(train_ix, batch_size=BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, batch_size=BATCH_SIZE)\n",
    "\n",
    "    n_train_words = sum(len(sent) for _, sent in train_ix)\n",
    "    n_valid_words = sum(len(sent) for _, sent in valid_ix)\n",
    "\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate loss.\n",
    "        valid_loss = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=False)\n",
    "            valid_loss += loss.value()\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train perplexity: {:8.3f} \"\n",
    "               \"Valid perplexity: {:8.3f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            exp(total_loss / n_train_words),\n",
    "            exp(valid_loss / n_valid_words)\n",
    "            ))\n",
    "        valid_perplexity_all_bigram.append(exp(valid_loss / n_valid_words))\n",
    "        \n",
    "    # FIXME: make sure to update filenames when implementing ngram models\n",
    "    fn = \"embeds_baseline_lm\"\n",
    "    if USE_UNLABELED:\n",
    "        fn += \"_unlabeled\"\n",
    "\n",
    "    print(\"Saving embeddings to {}\".format(fn))\n",
    "    lm.embed.save(fn, \"/embed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XucVNWd7/3PVzQCxngBBJVLoxJHJAS0MXFijGiOGmNE\niQnkwZmomRATTWLMxCcexsAxh3miZmIuczI5GgkaUMd4iegkSiJeJueoCHKRi4oTARsVECMKSFT6\n9/yxd2PTdFft3dXVVdX9fb9e+1W1V+219+pmU79el72WIgIzM7NC9qh0AczMrPo5WJiZWVEOFmZm\nVpSDhZmZFeVgYWZmRTlYmJlZUQ4WZmZWlIOFmZkV5WBhZmZF7VnpApSib9++UVdXV+limJnVlIUL\nF74aEf3y5KnpYFFXV8eCBQsqXQwzs5oiaU3ePG6GMjOzohwszMysKAcLMzMrqqb7LMws8c4779DQ\n0MD27dsrXRSrIj179mTgwIHstddeJZ/LwcKsC2hoaGDfffelrq4OSZUujlWBiGDTpk00NDQwdOjQ\nks/XPZuhZs+GujrYY4/kdfbsSpfIrCTbt2+nT58+DhS2kyT69OnTYbXNsgULSTMkbZC0rJXPvi0p\nJPVtlnaFpOclPSvptHKVi9mzYfJkWLMGIpLXyZMdMKzmOVBYSx15T5SzZjETOL1loqRBwKnA2mZp\nw4GJwNFpnp9L6lGWUk2ZAtu27Zq2bVuSbmZmrSpbsIiIR4HXWvnoOuByoPni3+OA2yLirxHxAvA8\ncFxZCrZ2bb50M8ukoaGBcePGMWzYMA4//HC++c1v8vbbbxfNV1dXx6uvvpr5Ou9///szHztt2jR+\n+MMfZj4+7/kLXWPatGkceuihjBo1imHDhjF+/HhWrFhR9HwzZ87kpZdeylWGztCpfRaSxgHrImJJ\ni48OBV5stt+QpnW8wYPzpZt1RR3cbxcRjB8/nrPPPptVq1bx3HPPsWXLFqZ08xr7t771LRYvXsyq\nVauYMGECJ598Mhs3biyYp9sHC0m9gf8OfK/E80yWtEDSgmK/9FZNnw69e++a1rt3km7WHZSh327e\nvHn07NmTCy64AIAePXpw3XXXMWPGDLZt28bMmTMZP348p59+OsOGDePyyy/f7Rzf+973+PGPf7xz\nf8qUKfzkJz/JdP17772Xj3zkI4wePZpPfvKTrF+/fudnS5Ys4fjjj2fYsGHccMMNO9OvvfZaxowZ\nw8iRI5k6dWqr523rmOnTp/PBD36QE044gWeffTZTGSdMmMCpp57KLbfcAsBVV13FmDFjGDFiBJMn\nTyYiuOOOO1iwYAGTJk1i1KhRvPXWW60eVxERUbYNqAOWpe8/BGwAVqfbuyT9FgOAK4ArmuV7ADi+\n2PmPPfbYaJdZsyKGDImQktdZs9p3HrMqsWLFiuwHDxkSkYSJXbchQ9p9/Z/85Cdx6aWX7pY+atSo\nWLJkSfzqV7+KoUOHxuuvvx5vvfVWDB48ONauXZsWZ0hs3LgxXnjhhRg9enREROzYsSMOO+ywePXV\nV3c75z777LNb2muvvRaNjY0REXHDDTfEZZddFhERU6dOjZEjR8a2bdti48aNMXDgwFi3bl088MAD\n8eUvfzkaGxtjx44d8elPfzoeeeSRXc7f1jELFiyIESNGxNatW2Pz5s1x+OGHx7XXXrtbmaZOnbpb\n+nXXXRcXXXRRRERs2rRpZ/p5550Xc+bMiYiIT3ziE/Hkk0/u/Kyt47Jq7d4AFkTO7/NOe84iIp4G\nDmral7QaqI+IVyXNAW6R9CPgEGAYML9shZk0KdnMuqMK9dudcsop7LfffgAMHz6cNWvWMGjQoJ2f\n19XV0adPHxYtWsT69esZPXo0ffr0yXTuhoYGJkyYwMsvv8zbb7+9y3MF48aNo1evXvTq1YuxY8cy\nf/58/vSnPzF37lxGjx4NwJYtW1i1ahUnnnjiznxz585t9Zg333yTc845h95pC8VZZ52V+XcQzWoF\nDz30ENdccw3btm3jtdde4+ijj+Yzn/nMbnmyHlduZQsWkm4FTgL6SmoApkbEja0dGxHLJd0OrCCp\ncVwcETvKVTazbm3w4KTpqbX0dho+fDh33HHHLmlvvPEGa9eu5YgjjuCpp55i77333vlZjx49ePfd\nd3c7zz/8wz8wc+ZMXnnlFS688MLM1//617/OZZddxllnncXDDz/MtGnTdn7WcvioJCKCK664gq98\n5SttnrOtY5o3leW1aNEi6uvr2b59O1/72tdYsGABgwYNYtq0aa0+D5H1uM5QztFQX4iIgyNir4gY\n2DJQRERdRLzabH96RBweEUdGxO/LVS6zbq8M/XannHIK27Zt4+abbwZgx44dfPvb3+b888/f+Rd4\nFueccw73338/Tz75JKedlv1xq82bN3PoocmYmJtuummXz+655x62b9/Opk2bePjhhxkzZgynnXYa\nM2bMYMuWLQCsW7eODRs27JKvrWNOPPFEfvvb3/LWW2/x5ptvcu+992Yq45133sncuXP5whe+sPML\nv2/fvmzZsmWXQLvvvvvy5ptvAhQ8rrN5ug+z7qapCXbKlKTpafDgJFCU0DQribvvvpuvfe1rfP/7\n36exsZEzzjiDf/7nf851nve9732MHTuW/fffnx49Wn/Uatu2bQwcOHDn/mWXXca0adP43Oc+xwEH\nHMDJJ5/MCy+8sPPzkSNHMnbsWF599VWuvPJKDjnkEA455BBWrlzJ8ccfDyTDZWfNmsVBB+1sKefU\nU09t9ZhjjjmGCRMm8OEPf5iDDjqIMWPGtPnzXHfddcyaNYutW7cyYsQI5s2bR79+yZpDX/7ylxkx\nYgQDBgzY5Rznn38+F110Eb169eKxxx5r87jOpuZtaLWmvr4+vPiRGaxcuZKjjjqq0sUoWWNjI8cc\ncwy/+c1vGDZsWKWL0yW0dm9IWhgR9XnO0z3nhjKzqrNixQqOOOIITjnlFAeKKuRmKDOrCsOHD+fP\nf/5zpYthbXDNwszMinKwMDOzohwszMysKAcLMzMrysHCzEq2evVqRowYsUtae6YHb+nhhx/mzDPP\n3CXt/PPPL/pw2syZM7nkkksKHuPpy/NxsDDrZgYMAGn3bcCASpese6m16csdLMy6mWazd2dK7wg/\n/elPGT58OCNHjmTixIkAbN26lQsvvJDjjjuO0aNHc8899+Q+b11dHVOnTuWYY47hQx/6EM8888xu\nx3j68o7hYGFmZfeDH/yARYsWsXTpUn7xi18AyZfqySefzPz583nooYf4zne+w9atW3Ofu2/fvjz1\n1FN89atfbbXJ54QTTuDxxx9n0aJFTJw4kWuuuWbnZ0uXLmXevHk89thjXHXVVbz00kvMnTuXVatW\nMX/+fBYvXszChQt59NFHdzlnW8csXLiQ2267jcWLF/O73/2OJ598MvPPccwxx+wMdpdccglPPvkk\ny5Yt46233uK+++7j3HPPpb6+ntmzZ7N48WJ69erV6nHl4ofyzKxkLWd2bZk+cuRIJk2axNlnn83Z\nZ58NJF+4c+bM2fkFv337dtauXbvL1BTFzgswfvx4AI499ljuuuuu3Y719OUdw8HCzErWp08f/vKX\nv+yS9tprr+38Yv6P//gPHn30Ue69916mT5/O008/TURw5513cuSRR+Y+b9++fXfuN0193ta0556+\nvGO4GcrMSvb+97+fgw8+mHnz5gHJF/r999/PCSecQGNjIy+++CJjx47l6quvZvPmzWzZsoXTTjuN\nn/3sZzv/ol60aNFu5x02bBgvvfQSK1euBGDNmjUsWbKEUaNGZS6bpy/vGK5ZmHUz/fu33pndv39p\n57355pu5+OKLueyyywCYOnUqhx9+OO+88w7nnXcemzdvJiL4xje+wf7778+VV17JpZdeysiRI2ls\nbGTo0KG7tbnvvffezJo1iwsuuIDt27ez11578ctf/nLnintZePryjuEpys26gK4yRbl1PE9RbmZm\nncbBwszMinKwMOsiarlJ2cqjI+8JBwuzLqBnz55s2rTJAcN2igg2bdpEz549O+R8RUdDSboG+J/A\nW8D9wEjgWxExq0NKYGYlGzhwIA0NDUXnFrLupWfPngwcOLBDzpVl6OypEXG5pHOA1cB44FHAwcKs\nSuy11167PJls1tGyNEM1BZRPA7+JiM1lLI+ZmVWhLDWL+yQ9Q9IM9VVJ/YDyPVNuZmZVp2jNIiK+\nC/wtUB8R7wBbgXHlLpiZmVWPNmsWksa3ktZ8d/fpHc3MrEsq1AxVaJ7bwMHCzKzbaDNYRMQFnVkQ\nMzOrXkX7LCT1l3SjpN+n+8MlfSlDvhmSNkha1izt+5KWSlosaa6kQ5p9doWk5yU9K+m09v5AZmbW\n8bIMnZ0JPAA0fbE/B1yaMd/pLdKujYiRETEKuA/4HiQBCJgIHJ3m+bmkHhmuYWZmnSBLsOgbEbcD\njQAR8S6wo1imiHgUeK1F2hvNdvch6fuAZHTVbRHx14h4AXgeOC5D2czMrBNkec5iq6Q+pF/skj4K\ntPvBPEnTgb9PzzE2TT4UeLzZYQ1pmpmZVYEsNYvLgDnA4ZL+D3Az8PX2XjAipkTEIGA2cEne/JIm\nS1ogaYHnwTEz6xxZHsp7CvgEyYN5XwGOjoilHXDt2cBn0/frgEHNPhuYprVWnusjoj4i6puWIDQz\ns/LK9VBe6oOSiIjcz1lIGhYRq9LdccAz6fs5wC2SfkTSkT4MmJ/3/GZmVh5ZHso7iKRWMS/dHwv8\nX4o8lCfpVuAkoK+kBmAqcIakI0k6y9cAFwFExHJJtwMrgHeBiyOiaCe6mZl1jqIP5UmaCwyPiJfT\n/YNJhsUWFBFfaCX5xgLHTwemFzuvmZl1viwd3IOaAkVqPTC4TOUxM7MqlGXo7IOSHgBuTfcnAH8s\nX5HMzKzaFA0WEXFJukreiWnS9RFxd3mLZWZm1SRLzQKSDu13SR7M8yglM7NuJstEgp8nCRDnAp8H\nnpB0brkLZmZm1SNLzWIKMCYiNgCky6r+EbijnAUzM7PqkWU01B5NgSK1KWM+MzPrIrLULO5vZTTU\n78pXJDMzqzZZRkN9R9JngY+lSR4NZWbWzWQaDRURdwJ3lrksZmZWpQpNJPgm7y1OtMtHQETEB8pW\nKjMzqyqFahYPAgNIJgz894hY0zlFMjOzatPmqKaIOBs4DdgIXC/pEUlfk3Rgp5XOzMyqQsEhsBGx\nOSJ+BXwK+N/AVcD5nVAuMzOrIgU7uCX9LfAF4OPAn4BzIuI/O6NgZmZWPQp1cK8GXgduAyaTzA2F\npGNg53KrZmbWDRSqWawmGQ11GnAqySioJgGcXL5imZlZNSm0Ut5JnVgOMzOrYp7jyczMinKwMDOz\nohwszMysqExzQ0kaCdQ1Pz4i7ipTmczMrMoUDRaSZgAjgeVAY5ocJNOAmJlZN5ClZvHRiBhe9pKY\nmVnVytJn8ZgkBwszs24sS7C4mSRgPCtpqaSnJS0td8Gq2uzZUFcHe+yRvM6eXekSmZmVVZZmqBuB\nvwOe5r0+i+5r9myYPBm2bUv216xJ9gEmTapcuczMyihLzWJjRMyJiBciYk3TVvaSVaspU94LFE22\nbUvSzcy6qCw1i0WSbgHuBf7alNhth86uXZsv3cysC8hSs+hFEiROBT6TbmcWyyRphqQNkpY1S7tW\n0jNp38fdkvZv9tkVkp5P+0ZOy/+jdJLBg/Olm5l1AUWDRURc0Mp2YYZzzwROb5H2B2BERIwEngOu\nAEhHW00Ejk7z/FxSjxw/R+eZPh169941rXfvJN3MrIvK8lBeT+BLJF/kPZvSiwWMiHhUUl2LtLnN\ndh8Hzk3fjwNui4i/Ai9Ieh44Dnis+I/QyZo6sadMSZqeBg9OAoU7t82sC8vSDPVrYADJuhaPAAOB\nNzvg2hcCv0/fHwq82OyzhjRtN5ImS1ogacHGjRs7oBjtMGkSrF4NjY3JqwOFmXVxWYLFERFxJbA1\nIm4CPg18pJSLSppCsvJe7gcUIuL6iKiPiPp+/fqVUgwzM8soy2iod9LX1yWNAF4BDmrvBSWdT9JB\nfkpERJq8DhjU7LCBaZqZmVWBLDWL6yUdAPwTMAdYAVzdnotJOh24HDgrIpo/rDAHmChpb0lDgWHA\n/PZcw8zMOl7BmoWkPYA3IuIvwKPAYVlPLOlW4CSgr6QGYCrJ6Ke9gT9IAng8Ii6KiOWSbicJRO8C\nF0fEjnb8PGZmVgZ6ryWojQOkBRFR30nlyaW+vj4WLFhQ6WKYmdUUSQvzfq9naYb6o6R/lDRI0oFN\nWzvLaGZmNShLB/eE9PXiZmlBjiYpMzOrbUWDRUQM7YyCmJlZ9cryBPf4VpI3A09HxIaOL5KZmVWb\nLM1QXwKOBx5K908CFgJDJV0VEb8uU9nMzKxKZAkWewJHRcR6AEn9SVbP+wjJcFoHCzOzLi7LaKhB\nTYEitSFNe433nu42M7MuLEvN4mFJ9wG/Sfc/m6btA7xetpKZmVnVyBIsLiYJEB9L928G7kzndRpb\nroKZmVn1yDJ0NoA70s3MzLqhNoOFpD9FxAmS3iR5CG/nRyQx5ANlL52ZmVWFNoNFRJyQvu7becUp\nvwEDYP363dP794dXXun88piZ1YIso6EAkNRbUr2kvuUsULm1FigKpZuZWYFgIeksSaslPSXpDGA5\n8K/AMklf7LQSmplZxRXq4P4+cCqwH8nT2yMj4s+SDgIeBG7qhPKZmVkVKBQsGiPiOQBJL0TEnwEi\nYoOkdzuldGZmVhUKBYs90uVU9wAa0/dq+qzsJTMzs6pRKFjsRzJhYFOAeKrZZ4WX16ti/fu3PRrK\nzMxaV2jobF0nlqPTeHismVl+bk6qhNmzoa4O9tgjeZ09u9IlMjMrKMvcUNaRZs+GyZNh27Zkf82a\nZB9g0qTKlcvMrADXLDrblCnvBYom27Yl6WZmVapgsJDUQ9IznVWYbmHt2nzpZmZVoGCwiIgdwLOS\nBndSebq+wW38KttKNzOrAlmaoQ4Alkt6UNKcpq3cBeuypk+H3r13TevdO0k3M6tSWTq4ryx7KbqT\npk7sKVOSpqfBg5NA4c5tM6tiStY2KnKQ1B8Yk+7Oj4gNZS1VRvX19bFgwYJKF8PMrKZIWhgR9Xny\nFG2GkvR5YD7wOeDzwBOSzm1fEc3MrBZl6bOYAoyJiC9GxN8Dx5GhaUrSDEkbJC1rlvY5ScslNUqq\nb3H8FZKel/SspNPy/iBmZlY+WYLFHi2anTZlzDcTOL1F2jJgPPBo80RJw4GJwNFpnp9L6pHhGmZm\n1gmydHDfL+kB4NZ0fwLwu2KZIuJRSXUt0lYCSGp5+Djgtoj4K/CCpOdJajCPZSifmZmVWdFgERHf\nkfRZ4GNp0vURcXcHl+NQ4PFm+w1pmpmZVYE2g4Wkj0bE4wARcSdwZ6eVqgBJk4HJAIP9IJuZWaco\n1Pfw86Y3ksrdHLQOGNRsf2CatpuIuD4i6iOivl+/fmUulpmZQeFg0bxjoWeZyzEHmChpb0lDgWEk\nw3XNzKwKZF1Wten9zgASEa8VOrGkW4GTgL6SGoCpwGvAz4B+wH9IWhwRp0XEckm3AyuAd4GL03mp\nzMysCrT5BLek1UAju9YwmkREHFbGcmXiJ7jNzPLr0Ce4I6IuIg6LiKGtbBUPFN2aV9ozs07mlfJq\njVfaM7MK8Ep5tcYr7ZlZBThY1BqvtGdmFZBl1tl/kXR0ZxTGMvBKe2ZWAVlqFiuB6yU9IekiSfuV\nu1BWgFfaM7MKKBosIuKXEfEx4O+BOmCppFskjS134awVkybB9dfDkCEgJa/XX+/ObTMrq0yjodLp\nwv8m3V4FlgCXSfpKREwsY/msNZMmOTiYWacqGiwkXQecCcwD/jkimqbhuFrSs+UsnJmZVYcsNYul\nwD9FxNZWPjuug8tjZmZVKEsH93ktA4WkBwEiYnNZSmVmZlWlzWAhqaekA0kmAjxA0oHpVocXJqpt\nni7EzHIq1Az1FeBS4BDgqWbpbwD/Ws5CWRl5uhAza4c2Z53deYD09Yj4WSeVJxfPOtsOdXVJgGhp\nyBBYvbqzS2NmFdCeWWcLLat6ckTMA9ZJGt/y84i4qx1ltErzdCFm1g6FmqE+QTJc9jOtfBaAg0Ut\nGjy49ZqFpwsxswLaDBYRMTV9vaDzimNlN336rn0W4OlCzKyoLBMJ/rr5fFCShjQNnbUa5OlCzKwd\nsjxn8SfgCUlnSPoy8Afgx+UtVvUaMCD5jm25DRhQ6ZLlMGlS0pnd2Ji85g0UHnpr1u0UfYI7Iv63\npOXAQyTzQo2OiFfKXrIqtX59vvQux0NvzbqlLM1QfwfMIJl1dibwO0kfLnO5rFp5pT6zbinL3FCf\nBU6IiA3ArZLuBm4CRpW1ZFadPPTWrFvKsp7F2WmgaNqfjycQ7L68Up9Zt5SlGeqDkh6UtCzdHwlc\nXvaSWXXySn1m3VKW0VA3AFcA7wBExFKg2y541L9/vvQupyOG3no0lVnNydJn0Tsi5ktqnvZumcpT\n9V7ptuPAmillpT6PpjKrSVlqFq9KOpxkig8knQu8XNZSWdfl0VRmNSlLzeJi4HrgbyStA14Azitr\nqazr8mgqs5qUZTTUnyPik0A/4G8i4oSIWF32klnX1BGjqdznYdbpCk1Rflkb6QBExI8KnVjSDOBM\nYENEjEjTDgT+HagDVgOfj4i/pJ9dAXwJ2AF8IyIeyPejWE0odSJD93mYVUShmsW+RbZiZgKnt0j7\nLvBgRAwDHkz3kTScZITV0Wmen0vqkfmnsNpR6mgq93mYVUTRlfJKOnmyXvd9zWoWzwInRcTLkg4G\nHo6II9NaBRHx/6XHPQBMi4jHCp3fK+V1Q3vsAa3ds1IyMWIWs2cnwWXt2qT5a/p010qsW2nPSnlZ\nHso7TNK9kjZK2iDpHkmHtbOM/SOiaSTVK0DT0wmHAi82O64hTTPbVal9Hk3NWGvWJEGnqRnL/R5m\nBWUZOnsLcDtwMHAI8Bvg1lIvHEmVJne1RtJkSQskLdi4cWOpxbBaU+oT5B3RjOUOduuGsgSL3hHx\n64h4N91mAT3beb31afMT6WvTnFPrgEHNjhuYpu0mIq6PiPqIqO/Xr187i2E1q9Q+j1KH7rpmYt1U\nlmDxe0nflVSXrpJ3Ock05Qemo5vymAN8MX3/ReCeZukTJe0taSgwDJif89zWXZSyeFOpzViumVg3\nleWhvM+nr19pkT6RpBmp1f4LSbcCJwF9JTUAU4EfALdL+hKwpuncEbFc0u3ACpKpRC6OiB35fhSz\nDEoduttRNRMP/bVaExFtbiQ1j48VOqaS27HHHhu1pn//iKT9Ytetf/9Kl6wbmTUrYsiQCCl5nTUr\ne94hQ1r/BxwypHPyl1p+s4gAFkTO79uCzVAR0Qj8ayfErG6j2y/LWg1KacYqtYO9GvpM3Axm7ZCl\nz+JBSZ9Vi2lnzbqlUjvYK91nUulg40BVu4pVPYA3gUaS9SzeSPffyFuFKcdWi81QrbVANG3WDcya\nFdG7967/8L17Z29Kklq/eaRs+UttBiul/KX+7E3nKKUJzk14EdG+ZqiKf+GXsjlYWE2qZJ9JJYNN\nJQNVR+RvOkclg1UHBbuyBAtAJFOSX5nuDwKOy3uhcmwOFtbtlPqFV8lgU+laUa0Hq44IdqlyBYt/\nA/4XsDLdPwB4Mu+FyrHVYrDwaCgrWSl/XVYy2FS6VlTrwaojRtKlyhUsnkpfFzVLW5L3QuXYajFY\nmFVcpYJNpWtFtR6sSs3fTHuCRZbRUO+k04UHgKR+aYe3mdWiUoYOlzIarNSRZKUOWy41f6kj2Sqd\nv1TFogkwiWQ6jgZgOvAs8Lm8Uakcm2sWZt1MJTuIK93nUO19Fsl5+RuStbgvAY7Ke5Fybd0xWLjP\nw6yCKj2aqYKjodpc/EhST+Ai4AjgaeDGiHi3rNWcnLrj4keFHo1s45/SzGwXHb340U1APUmg+BTw\nwxLKZmZmNazQrLPDI+JDAJJuxFOGm5l1W4VqFu80vam25iczM+tchWoWH5b0RvpeQK90X0BExAfK\nXjozM6sKbQaLiOjRmQWxbPr3b3068/79O78sZtZ9ZHkoz6rIK6+0NnA2Sc9iwIBkRFXLbcCA8pbb\nzGqbg0U348WXzKw9HCzMzKwoBwszMyvKwcLMzIpysLBc3EFu1j05WHQzbQ2xzTr01h3kZt1ToYfy\nrAvKOsTWzKw51yysU7kZy6w2OVhYp3IzllltcrAwM7OiHCwsl1I7yEvlZiyzynAHt+VS6Q5yN2OZ\nVUZFahaSvilpmaTlki5N0w6U9AdJq9LXAypRNuvaXDMxa59ODxaSRgBfBo4DPgycKekI4LvAgxEx\nDHgw3bcuptLNWK6ZmLVPJWoWRwFPRMS2dAW+R4DxwDiSdb9JX8+uQNmszEqdYr3SXDOx7qoSwWIZ\n8HFJfST1Bs4ABgH9I+Ll9JhXAC/nY1Wn1JqJg43Vqk4PFhGxErgamAvcDywGdrQ4JoBoLb+kyZIW\nSFqwcePGchfXqkylm7FK5WBjtaoiHdwRcWNEHBsRJwJ/AZ4D1ks6GCB93dBG3usjoj4i6vv169d5\nhbaqUOvNWKVysLFKqdRoqIPS18Ek/RW3AHOAL6aHfBG4pxJls66t1msmpapksHGgqm2VeijvTkkr\ngHuBiyPideAHwH+TtAr4ZLpv1qG6e82kVKUEm0rXiiqdv9ZV5KG8iPh4K2mbgFMqUByzzPr3b/3L\nrbvUTCqp1GBT6fwDBrR972T5Y6XU/KXyE9xmOZT6n9LBpvuqdLAqlYOFWSdysLFa5WBhVkMcbKxS\nPOusWTdSagd/KaPJuvtItFrnYGFmmZUSbCoZqKohf61zM5SZ1YRSm+Aqnb/UJsBKNyE6WJiZdYJK\nB6tSuRnKzMyKcrAwM7OiHCzMzKwoBwszMyvKwcLMzIpSss5QbZL0JvBsCafoC7zq/M7v/J2ev5bL\n3hXyHxkR++bKERE1uwELnN/5nb/28tdy2btrfjdDmZlZUQ4WZmZWVK0Hi+ud3/mdvybz13LZu2X+\nmu7gNjOzzlHrNQszM+sENR8sJH1O0nJJjZLqc+Q7XdKzkp6X9N2c15whaYOkZflLDJIGSXpI0oq0\n7N/Mmb9ymdjTAAAJwElEQVSnpPmSlqT5/0c7ytBD0iJJ9+XNm+ZfLelpSYslLciZd39Jd0h6RtJK\nScfnyHtkes2m7Q1Jl+a8/rfS39sySbdK6pkz/zfTvMuzXLu1+0XSgZL+IGlV+npAzvyZ7/s28l+b\n/v6XSrpb0v45838/zbtY0lxJh+TJ3+yzb0sKSX1zXn+apHXN7oMz8l5f0tfT38FySdfkvP6/N7v2\nakmLc+YfJenxpv8/ko7Lmf/Dkh5L/w/eK+kDBfK3+n2T5x4EanvobNqEdhRwJPAwUJ8xTw/gv4DD\ngPcBS4DhOa55InAMsKydZT4YOCZ9vy/wXM7rC3h/+n4v4AngoznLcBlwC3BfO3+G1UDfdua9CfiH\n9P37gP3beZ4ewCvAkBx5DgVeAHql+7cD5+fIPwJYBvQmmbX5j8ARee8X4Brgu+n77wJX58yf+b5v\nI/+pwJ7p+6vbcf0PNHv/DeAXefKn6YOAB4A1he6lNq4/DfjHjP9mreUfm/7b7Z3uH5S3/M0+/xfg\nezmvPxf4VPr+DODhnPmfBD6Rvr8Q+H6B/K1+3+S5ByO6wNDZiFgZEXkfzDsOeD4i/hwRbwO3AeNy\nXPNR4LWc12ye/+WIeCp9/yawkuRLLGv+iIgt6e5e6Za580nSQODTwC8zF7qDSNqP5Oa/ESAi3o6I\n19t5ulOA/4qINTnz7Qn0krQnyZf+SznyHgU8ERHbIuJd4BFgfKEMbdwv40iCJunr2Xny57nv28g/\nNy0/wOPAwJz532i2uw8F7r8C/1+uAy4vlLdI/kzayP9V4AcR8df0mA3tub4kAZ8Hbs2ZP4Cm2sB+\nFLgH28j/QeDR9P0fgM8WyN/W903mexC6QDNUOx0KvNhsv4EcX9YdSVIdMJqkdpAnX4+06rsB+ENE\n5Mn/Y5L/pI15rtlCAH+UtFDS5Bz5hgIbgV+lzWC/lLRPO8swkQL/SVsTEeuAHwJrgZeBzRExN8cp\nlgEfl9RHUm+SvwoH5SlDqn9EvJy+fwWo5HprFwK/z5tJ0nRJLwKTgO/lzDsOWBcRS/Jet5mvp01h\nM4o2oezugyT/jk9IekTSmHaW4ePA+ohYlTPfpcC16e/vh8AVOfMv570/cD9HxnuwxfdNrnuwJoKF\npD+mbcQtt8y1gWok6f3AncClLf5SKyoidkTEKJK/CI+TNCLjNc8ENkTEwtwF3tUJ6fU/BVws6cSM\n+fYkqVL/W0SMBraSVIFzkfQ+4CzgNznzHUDyn2wocAiwj6TzsuaPiJUkzTZzgfuBxcCOPGVo5ZxB\njpphR5I0BXgXmJ03b0RMiYhBad5LclyzN/DfyRlgWvg3kmbkUSRB/19y5t8TOBD4KPAd4Pa0lpDX\nF8j5B0vqq8C30t/ft0hr2jlcCHxN0kKSpqW3i2Uo9H2T5R6siWAREZ+MiBGtbPe085Tr2DUSD0zT\nOo2kvUj+4WZHxF3tPU/ahPMQcHrGLB8DzpK0mqT57WRJs9px3XXp6wbgbpKmvSwagIZmNaE7SIJH\nXp8CnoqIVhaaLOiTwAsRsTEi3gHuAv42zwki4saIODYiTgT+QtIGnNd6SQcDpK9tNoOUi6TzgTOB\nSemXRXvNpkAzSCsOJwnWS9L7cCDwlKQBWU8QEevTP5gagRvIfv81aQDuSpt055PUstvsZG9N2ow5\nHvj3nNcG+CLJvQfJHzy5yh8Rz0TEqRFxLEmw+q8iZW3t+ybXPVgTwaIMngSGSRqa/oU6EZjTWRdP\n/4K5EVgZET9qR/5+SkevSOoF/DfgmSx5I+KKiBgYEXUkP/e8iMj8l3V6zX0k7dv0nqSzNNPIsIh4\nBXhR0pFp0inAijzXT7X3L7q1wEcl9U7/HU4hacPNTNJB6etgki+LW9pRjjkkXxikr+39w6ddJJ1O\n0hR5VkRsa0f+Yc12x5Hx/gOIiKcj4qCIqEvvwwaSDtjMC4c2fcmlziHj/dfMb0k6uZH0QZKBFnkn\n5vsk8ExENOTMB0kfxSfS9ycDuZqxmt2DewD/BPyiwLFtfd/kuwcL9X7XwkZyozQAfwXWAw9kzHcG\nyV+E/wVMyXnNW0mqvu+k1/5SzvwnkFT5lpI0YywGzsiRfySwKM2/jAIjMYqc5yTaMRqKpPq/JN2W\nt+P3NwpYkJb/t8ABOfPvA2wC9mvnz/0/SL7clgG/Jh0RkyP/f5IEuCXAKe25X4A+wIMkXxJ/BA7M\nmT/zfd9G/udJ+u2a7r9Co5lay39n+vtbCtwLHNre/y8UGVnXxvV/DTydXn8OcHDO/O8DZqU/w1PA\nyXnLD8wELmrnv/8JwML0HnoCODZn/m+SfH89B/yA9AHrNvK3+n2T5x6MCD/BbWZmxXXXZigzM8vB\nwcLMzIpysDAzs6IcLMzMrCgHCzMzK8rBwroMSTu064y0uZ8ML3DuOmWYZVjJbKjbmsbBp2lbCuXp\n6DKYlcOelS6AWQd6K5IpSCrtVeDbwP9b6YI0J2nPeG/yQLNcXLOwLi9db+CadO7/+ZKOSNPrJM1L\nJ6N7MH0iG0n9lazxsCTdmqYD6SHphnRNgLnp0/OtmQFMkHRgi3LsUjOQ9I+SpqXvH5Z0nZK1DVZK\nGiPpLiVrDfzPZqfZU9Ls9Jg70nmWkHRsOiHeQkkPNJvG4WFJP1ay5kiudVPMmnOwsK6kV4tmqAnN\nPtscER8C/pVk1l2AnwE3RcRIkvmNfpqm/xR4JCI+TDJv1fI0fRjwvyLiaOB12p4PaQtJwMj75fx2\nRNSTTN1wD3AxyfoZ50vqkx5zJPDziDgKeINkMrm90p/l3EjmCpoBTG923vdFRH1E5J1sz2wnN0NZ\nV1KoGerWZq/Xpe+P5721KH5NshgMJHP1/D0ks/sCm9PZal+IiKYV0RYCdQXK8lNgsaQf5ih/0/xk\nTwPLI50+WtKfSSa+fB14MSL+T3rcLJKFh+4nCSp/SCdO7UEyPUST9kx0Z7YLBwvrLqKN93n8tdn7\nHUBbzVBExOuSbiGpHTR5l11r8y2Xc206f2OLazXy3v/VlmUPkpUTl0dEW8vTbm2rnGZZuRnKuosJ\nzV4fS9//X5KZdyFZwOc/0/cPkqw30LTI1H7tvOaPgK/w3hf9euAgJQsn7U0yPXheg/XemuX/D/An\n4FmgX1O6pL0kHd3OMpu1ysHCupKWfRY/aPbZAZKWkvQjfCtN+zpwQZr+d7zXx/BNYKykp0mam4a3\npzAR8SrJWh97p/vvAFcB80mWwsw8rXczz5IsNrUSOIBkEam3gXOBqyUtIZlVNNcaHWbFeNZZ6/LS\nBXbq0y9vM2sH1yzMzKwo1yzMzKwo1yzMzKwoBwszMyvKwcLMzIpysDAzs6IcLMzMrCgHCzMzK+r/\nBy+V98HjQPP+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ee1d320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 5\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(valid_perplexity_labeled_bigram, 'ro', label='Only Labeled Data')\n",
    "plt.plot(valid_perplexity_all_bigram, 'bs', label='Use Unlabeled Data')\n",
    "plt.ylabel('Perplexity For Bigram Models')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.legend()\n",
    "plt.xticks(range(-1, 21))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 28.1s. Train perplexity:  195.445 Valid perplexity:  123.347\n",
      "Epoch   1 took 27.0s. Train perplexity:  118.824 Valid perplexity:  102.882\n",
      "Epoch   2 took 28.7s. Train perplexity:  101.632 Valid perplexity:   92.863\n",
      "Epoch   3 took 28.5s. Train perplexity:   91.366 Valid perplexity:   87.075\n",
      "Epoch   4 took 28.1s. Train perplexity:   84.504 Valid perplexity:   83.420\n",
      "Epoch   5 took 28.1s. Train perplexity:   79.498 Valid perplexity:   80.935\n",
      "Epoch   6 took 29.0s. Train perplexity:   75.588 Valid perplexity:   79.185\n",
      "Epoch   7 took 27.1s. Train perplexity:   72.405 Valid perplexity:   77.916\n",
      "Epoch   8 took 28.3s. Train perplexity:   69.737 Valid perplexity:   76.984\n",
      "Epoch   9 took 28.6s. Train perplexity:   67.458 Valid perplexity:   76.301\n",
      "Epoch  10 took 28.4s. Train perplexity:   65.473 Valid perplexity:   75.803\n",
      "Epoch  11 took 28.4s. Train perplexity:   63.712 Valid perplexity:   75.446\n",
      "Epoch  12 took 29.0s. Train perplexity:   62.130 Valid perplexity:   75.202\n",
      "Epoch  13 took 27.3s. Train perplexity:   60.698 Valid perplexity:   75.053\n",
      "Epoch  14 took 27.3s. Train perplexity:   59.389 Valid perplexity:   74.982\n",
      "Epoch  15 took 28.7s. Train perplexity:   58.187 Valid perplexity:   74.978\n",
      "Epoch  16 took 26.2s. Train perplexity:   57.079 Valid perplexity:   75.031\n",
      "Epoch  17 took 25.9s. Train perplexity:   56.055 Valid perplexity:   75.132\n",
      "Epoch  18 took 26.9s. Train perplexity:   55.104 Valid perplexity:   75.272\n",
      "Epoch  19 took 27.8s. Train perplexity:   54.218 Valid perplexity:   75.446\n",
      "Saving embeddings to embeds_baseline_lm_trigram\n"
     ]
    }
   ],
   "source": [
    "# Task 6 trigrams\n",
    "\"\"\"Simplest possible neural language model:\n",
    "    use word w_i to predict word w_(i + 1)\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "from math import exp\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "USE_UNLABELED = False\n",
    "VOCAB_SIZE = len(vocab) #__FIXME__\n",
    "valid_perplexity_labeled_trigram = []\n",
    "valid_perplexity__trigram = []\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class SimpleNLM(object):\n",
    "\n",
    "    def __init__(self, n, params, vocab_size, hidden_dim):\n",
    "        self.ngram = n\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim * (n - 1)\n",
    "\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "\n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim * (n - 1)))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.W_out = params.add_parameters((vocab_size, hidden_dim))\n",
    "    \n",
    "    def batch_loss(self, batch, train=True):\n",
    "\n",
    "        # load the parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "\n",
    "        W_out = dy.parameter(self.W_out)\n",
    "        n = self.ngram\n",
    "        \n",
    "        losses = []\n",
    "        for _, sent in batch:\n",
    "            for i in range(n - 1, len(sent)):\n",
    "                #prev_word_ix = [] # Task 6: ck is with dimension n x (k - 1)\n",
    "                ctx = []\n",
    "                for j in range(i - n + 1, i):\n",
    "                    prev_word_ix = sent[j]\n",
    "                    ctx.append(dy.lookup(self.embed, prev_word_ix))\n",
    "                curr_word_ix = sent[i]\n",
    "                \n",
    "                ctx = dy.concatenate(ctx)\n",
    "                #ctx = dy.lookup(self.embed, prev_word_ix)\n",
    "\n",
    "                # hid is the hidden layer output, size=hidden_size\n",
    "                # compute b_hid + W_hid * ctx, but faster\n",
    "                hid = dy.affine_transform([b_hid, W_hid, ctx])\n",
    "                hid = dy.tanh(hid)\n",
    "\n",
    "                # out is the prediction of the next word, size=vocab_size\n",
    "                out = W_out * hid\n",
    "\n",
    "                # Intepretation: The model estimates that\n",
    "                # log P(curr_word=k | prev_word) ~ out[k]\n",
    "                # in other words,\n",
    "                # P(curr_word=k | prev_word) = exp(out[k]) / sum_j exp(out[j])\n",
    "                #                            = softmax(out)[k]\n",
    "\n",
    "                # We want to maximize the probability of the correct word.\n",
    "                # (equivalently, minimize the negative log-probability)\n",
    "\n",
    "                loss = dy.pickneglogsoftmax(out, curr_word_ix)\n",
    "                losses.append(loss)\n",
    "\n",
    "        # esum simply adds up the expressions in the list\n",
    "        return dy.esum(losses)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "    if USE_UNLABELED:\n",
    "        #__FIXME__\n",
    "        with open(os.path.join('processed', 'unlab_ix.pkl'), 'rb') as f:\n",
    "            train_ix += pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    lm = SimpleNLM(3, params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    train_batches = make_batches(train_ix, batch_size=BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, batch_size=BATCH_SIZE)\n",
    "\n",
    "    n_train_words = sum(len(sent) for _, sent in train_ix)\n",
    "    n_valid_words = sum(len(sent) for _, sent in valid_ix)\n",
    "\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate loss.\n",
    "        valid_loss = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=False)\n",
    "            valid_loss += loss.value()\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train perplexity: {:8.3f} \"\n",
    "               \"Valid perplexity: {:8.3f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            exp(total_loss / n_train_words),\n",
    "            exp(valid_loss / n_valid_words)\n",
    "            ))\n",
    "        valid_perplexity_labeled_trigram.append(exp(valid_loss / n_valid_words))\n",
    "        \n",
    "    # FIXME: make sure to update filenames when implementing ngram models\n",
    "    fn = \"embeds_baseline_lm_trigram\"\n",
    "    if USE_UNLABELED:\n",
    "        fn += \"_unlabeled\"\n",
    "\n",
    "    print(\"Saving embeddings to {}\".format(fn))\n",
    "    lm.embed.save(fn, \"/embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 48.3s. Train perplexity:  138.406 Valid perplexity:   94.633\n",
      "Epoch   1 took 45.4s. Train perplexity:   92.042 Valid perplexity:   81.580\n",
      "Epoch   2 took 46.8s. Train perplexity:   81.031 Valid perplexity:   75.734\n",
      "Epoch   3 took 47.8s. Train perplexity:   74.982 Valid perplexity:   72.534\n",
      "Epoch   4 took 47.9s. Train perplexity:   71.031 Valid perplexity:   70.519\n",
      "Epoch   5 took 46.2s. Train perplexity:   68.198 Valid perplexity:   69.178\n",
      "Epoch   6 took 44.9s. Train perplexity:   66.039 Valid perplexity:   68.230\n",
      "Epoch   7 took 47.5s. Train perplexity:   64.305 Valid perplexity:   67.540\n",
      "Epoch   8 took 46.9s. Train perplexity:   62.868 Valid perplexity:   67.030\n",
      "Epoch   9 took 47.6s. Train perplexity:   61.647 Valid perplexity:   66.653\n",
      "Epoch  10 took 47.3s. Train perplexity:   60.594 Valid perplexity:   66.373\n",
      "Epoch  11 took 46.5s. Train perplexity:   59.671 Valid perplexity:   66.163\n",
      "Epoch  12 took 43.9s. Train perplexity:   58.854 Valid perplexity:   66.017\n",
      "Epoch  13 took 46.1s. Train perplexity:   58.127 Valid perplexity:   65.920\n",
      "Epoch  14 took 48.5s. Train perplexity:   57.474 Valid perplexity:   65.858\n",
      "Epoch  15 took 45.5s. Train perplexity:   56.884 Valid perplexity:   65.824\n",
      "Epoch  16 took 47.6s. Train perplexity:   56.350 Valid perplexity:   65.811\n",
      "Epoch  17 took 48.1s. Train perplexity:   55.863 Valid perplexity:   65.809\n",
      "Epoch  18 took 47.0s. Train perplexity:   55.415 Valid perplexity:   65.815\n",
      "Epoch  19 took 48.7s. Train perplexity:   55.000 Valid perplexity:   65.825\n",
      "Saving embeddings to embeds_baseline_lm_trigram_unlabeled\n"
     ]
    }
   ],
   "source": [
    "# Task 6 trigrams with unlabeled data\n",
    "\n",
    "USE_UNLABELED = True\n",
    "valid_perplexity_all_trigram = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "    if USE_UNLABELED:\n",
    "        #__FIXME__\n",
    "        with open(os.path.join('processed', 'unlab_ix.pkl'), 'rb') as f:\n",
    "            train_ix += pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    lm = SimpleNLM(3, params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    train_batches = make_batches(train_ix, batch_size=BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, batch_size=BATCH_SIZE)\n",
    "\n",
    "    n_train_words = sum(len(sent) for _, sent in train_ix)\n",
    "    n_valid_words = sum(len(sent) for _, sent in valid_ix)\n",
    "\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate loss.\n",
    "        valid_loss = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=False)\n",
    "            valid_loss += loss.value()\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train perplexity: {:8.3f} \"\n",
    "               \"Valid perplexity: {:8.3f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            exp(total_loss / n_train_words),\n",
    "            exp(valid_loss / n_valid_words)\n",
    "            ))\n",
    "        valid_perplexity_all_trigram.append(exp(valid_loss / n_valid_words))\n",
    "        \n",
    "    # FIXME: make sure to update filenames when implementing ngram models\n",
    "    fn = \"embeds_baseline_lm_trigram\"\n",
    "    if USE_UNLABELED:\n",
    "        fn += \"_unlabeled\"\n",
    "\n",
    "    print(\"Saving embeddings to {}\".format(fn))\n",
    "    lm.embed.save(fn, \"/embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKalYiIhISioWIiKS0v8HmF4Lhmvniz8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d6c76d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 6 trigram graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(valid_perplexity_labeled_trigram, 'ro', label='Only Labeled Data')\n",
    "plt.plot(valid_perplexity_all_trigram, 'bs', label='Use Unlabeled Data')\n",
    "plt.ylabel('Perplexity For Trigram Models')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.legend()\n",
    "plt.xticks(range(-1, 21))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 28.3s. Train perplexity:  163.895 Valid perplexity:  105.243\n",
      "Epoch   1 took 29.9s. Train perplexity:  100.772 Valid perplexity:   87.161\n",
      "Epoch   2 took 28.8s. Train perplexity:   86.746 Valid perplexity:   79.503\n",
      "Epoch   3 took 26.4s. Train perplexity:   78.989 Valid perplexity:   74.650\n",
      "Epoch   4 took 29.5s. Train perplexity:   73.637 Valid perplexity:   71.503\n",
      "Epoch   5 took 31.3s. Train perplexity:   69.675 Valid perplexity:   69.259\n",
      "Epoch   6 took 29.1s. Train perplexity:   66.397 Valid perplexity:   67.508\n",
      "Epoch   7 took 27.9s. Train perplexity:   63.626 Valid perplexity:   66.176\n",
      "Epoch   8 took 27.5s. Train perplexity:   61.234 Valid perplexity:   65.124\n",
      "Epoch   9 took 27.1s. Train perplexity:   59.096 Valid perplexity:   64.282\n",
      "Epoch  10 took 27.8s. Train perplexity:   57.218 Valid perplexity:   63.666\n",
      "Epoch  11 took 27.7s. Train perplexity:   55.552 Valid perplexity:   63.207\n",
      "Epoch  12 took 27.8s. Train perplexity:   54.048 Valid perplexity:   62.888\n",
      "Epoch  13 took 28.2s. Train perplexity:   52.688 Valid perplexity:   62.690\n",
      "Epoch  14 took 28.0s. Train perplexity:   51.445 Valid perplexity:   62.582\n",
      "Epoch  15 took 27.2s. Train perplexity:   50.301 Valid perplexity:   62.542\n",
      "Epoch  16 took 26.3s. Train perplexity:   49.239 Valid perplexity:   62.556\n",
      "Epoch  17 took 27.4s. Train perplexity:   48.244 Valid perplexity:   62.613\n",
      "Epoch  18 took 27.3s. Train perplexity:   47.311 Valid perplexity:   62.708\n",
      "Epoch  19 took 27.9s. Train perplexity:   46.434 Valid perplexity:   62.833\n",
      "Saving embeddings to embeds_baseline_lm_4gram\n"
     ]
    }
   ],
   "source": [
    "# Task 6 4-grams\n",
    "\n",
    "USE_UNLABELED = False\n",
    "valid_perplexity_labeled_4gram = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "    if USE_UNLABELED:\n",
    "        #__FIXME__\n",
    "        with open(os.path.join('processed', 'unlab_ix.pkl'), 'rb') as f:\n",
    "            train_ix += pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    lm = SimpleNLM(4, params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    train_batches = make_batches(train_ix, batch_size=BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, batch_size=BATCH_SIZE)\n",
    "\n",
    "    n_train_words = sum(len(sent) for _, sent in train_ix)\n",
    "    n_valid_words = sum(len(sent) for _, sent in valid_ix)\n",
    "\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate loss.\n",
    "        valid_loss = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=False)\n",
    "            valid_loss += loss.value()\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train perplexity: {:8.3f} \"\n",
    "               \"Valid perplexity: {:8.3f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            exp(total_loss / n_train_words),\n",
    "            exp(valid_loss / n_valid_words)\n",
    "            ))\n",
    "        valid_perplexity_labeled_4gram.append(exp(valid_loss / n_valid_words))\n",
    "        \n",
    "    # FIXME: make sure to update filenames when implementing ngram models\n",
    "    fn = \"embeds_baseline_lm_4gram\"\n",
    "    if USE_UNLABELED:\n",
    "        fn += \"_unlabeled\"\n",
    "\n",
    "    print(\"Saving embeddings to {}\".format(fn))\n",
    "    lm.embed.save(fn, \"/embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 47.4s. Train perplexity:  118.949 Valid perplexity:   82.677\n",
      "Epoch   1 took 44.1s. Train perplexity:   79.539 Valid perplexity:   70.120\n",
      "Epoch   2 took 48.1s. Train perplexity:   69.642 Valid perplexity:   64.678\n",
      "Epoch   3 took 47.4s. Train perplexity:   64.041 Valid perplexity:   61.412\n",
      "Epoch   4 took 46.9s. Train perplexity:   60.281 Valid perplexity:   59.320\n",
      "Epoch   5 took 48.4s. Train perplexity:   57.549 Valid perplexity:   57.936\n",
      "Epoch   6 took 47.8s. Train perplexity:   55.442 Valid perplexity:   56.941\n",
      "Epoch   7 took 47.5s. Train perplexity:   53.772 Valid perplexity:   56.230\n",
      "Epoch   8 took 46.9s. Train perplexity:   52.400 Valid perplexity:   55.711\n",
      "Epoch   9 took 45.3s. Train perplexity:   51.241 Valid perplexity:   55.326\n",
      "Epoch  10 took 47.6s. Train perplexity:   50.236 Valid perplexity:   55.028\n",
      "Epoch  11 took 47.7s. Train perplexity:   49.362 Valid perplexity:   54.790\n",
      "Epoch  12 took 46.3s. Train perplexity:   48.596 Valid perplexity:   54.609\n",
      "Epoch  13 took 46.8s. Train perplexity:   47.912 Valid perplexity:   54.473\n",
      "Epoch  14 took 47.8s. Train perplexity:   47.296 Valid perplexity:   54.371\n",
      "Epoch  15 took 45.8s. Train perplexity:   46.740 Valid perplexity:   54.295\n",
      "Epoch  16 took 47.5s. Train perplexity:   46.237 Valid perplexity:   54.245\n",
      "Epoch  17 took 47.2s. Train perplexity:   45.780 Valid perplexity:   54.215\n",
      "Epoch  18 took 48.5s. Train perplexity:   45.362 Valid perplexity:   54.201\n",
      "Epoch  19 took 47.8s. Train perplexity:   44.978 Valid perplexity:   54.197\n",
      "Saving embeddings to embeds_baseline_lm_4gram_unlabeled\n"
     ]
    }
   ],
   "source": [
    "# Task 6 trigrams with unlabeled\n",
    "\n",
    "USE_UNLABELED = True\n",
    "valid_perplexity_all_4gram = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "    if USE_UNLABELED:\n",
    "        #__FIXME__\n",
    "        with open(os.path.join('processed', 'unlab_ix.pkl'), 'rb') as f:\n",
    "            train_ix += pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    lm = SimpleNLM(4, params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    train_batches = make_batches(train_ix, batch_size=BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, batch_size=BATCH_SIZE)\n",
    "\n",
    "    n_train_words = sum(len(sent) for _, sent in train_ix)\n",
    "    n_valid_words = sum(len(sent) for _, sent in valid_ix)\n",
    "\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate loss.\n",
    "        valid_loss = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = lm.batch_loss(batch, train=False)\n",
    "            valid_loss += loss.value()\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train perplexity: {:8.3f} \"\n",
    "               \"Valid perplexity: {:8.3f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            exp(total_loss / n_train_words),\n",
    "            exp(valid_loss / n_valid_words)\n",
    "            ))\n",
    "        valid_perplexity_all_4gram.append(exp(valid_loss / n_valid_words))\n",
    "        \n",
    "    # FIXME: make sure to update filenames when implementing ngram models\n",
    "    fn = \"embeds_baseline_lm_4gram\"\n",
    "    if USE_UNLABELED:\n",
    "        fn += \"_unlabeled\"\n",
    "\n",
    "    print(\"Saving embeddings to {}\".format(fn))\n",
    "    lm.embed.save(fn, \"/embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1308f2b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 6 4-grams graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(valid_perplexity_labeled_4gram, 'ro', label='Only Labeled Data')\n",
    "plt.plot(valid_perplexity_all_4gram, 'bs', label='Use Unlabeled Data')\n",
    "plt.ylabel('Perplexity For 4-gram Models')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.xticks(range(-1, 21))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 2.1s. Train loss:  0.68899 Valid accuracy:    56.46\n",
      "Update parameters\n",
      "Epoch   1 took 2.3s. Train loss:  0.67866 Valid accuracy:    57.22\n",
      "Update parameters\n",
      "Epoch   2 took 2.0s. Train loss:  0.67112 Valid accuracy:    58.58\n",
      "Update parameters\n",
      "Epoch   3 took 2.0s. Train loss:  0.65927 Valid accuracy:    59.23\n",
      "Update parameters\n",
      "Epoch   4 took 2.3s. Train loss:  0.64471 Valid accuracy:    59.85\n",
      "Update parameters\n",
      "Epoch   5 took 1.9s. Train loss:  0.63040 Valid accuracy:    60.13\n",
      "Update parameters\n",
      "Epoch   6 took 1.8s. Train loss:  0.61786 Valid accuracy:    60.52\n",
      "Update parameters\n",
      "Epoch   7 took 1.9s. Train loss:  0.59933 Valid accuracy:    60.67\n",
      "Update parameters\n",
      "Epoch   8 took 2.1s. Train loss:  0.59194 Valid accuracy:    61.05\n",
      "Update parameters\n",
      "Epoch   9 took 1.8s. Train loss:  0.58187 Valid accuracy:    60.75\n",
      "Epoch  10 took 2.4s. Train loss:  0.56524 Valid accuracy:    60.97\n",
      "Epoch  11 took 2.2s. Train loss:  0.55450 Valid accuracy:    60.59\n",
      "Epoch  12 took 2.0s. Train loss:  0.54145 Valid accuracy:    61.11\n",
      "Update parameters\n",
      "Epoch  13 took 2.0s. Train loss:  0.53701 Valid accuracy:    61.16\n",
      "Update parameters\n",
      "Epoch  14 took 2.1s. Train loss:  0.52558 Valid accuracy:    60.77\n",
      "Epoch  15 took 2.3s. Train loss:  0.52015 Valid accuracy:    60.79\n",
      "Epoch  16 took 2.0s. Train loss:  0.51267 Valid accuracy:    61.14\n",
      "Epoch  17 took 2.3s. Train loss:  0.50486 Valid accuracy:    60.66\n",
      "Epoch  18 took 2.4s. Train loss:  0.49433 Valid accuracy:    60.66\n",
      "Epoch  19 took 2.1s. Train loss:  0.49088 Valid accuracy:    60.67\n",
      "Running model on test set. Test accuracy:    59.48\n"
     ]
    }
   ],
   "source": [
    "# Task 7 bigram embedding\n",
    "\"\"\"Simple deep averaging net classifier\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class DANClassifier(object):\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "        \n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.w_clf = params.add_parameters((1, hidden_dim))\n",
    "        self.b_clf = params.add_parameters((1))\n",
    "        \n",
    "        # Parameters used for test set\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "\n",
    "\n",
    "    def _predict(self, batch, train=True):\n",
    "\n",
    "        # load the network parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "        w_clf = dy.parameter(self.w_clf)\n",
    "        b_clf = dy.parameter(self.b_clf)\n",
    "\n",
    "        probas = []\n",
    "        # predict the probability of positive sentiment for each sentence\n",
    "        for _, sent in batch:\n",
    "            sent_embed = [dy.lookup(self.embed, w) for w in sent]\n",
    "            # Task 3\n",
    "            if train == True:\n",
    "                for i in range(len(sent_embed)):\n",
    "                    sent_embed[i] = dy.dropout(sent_embed[i], 0.5)\n",
    "\n",
    "            sent_embed = dy.average(sent_embed)\n",
    "\n",
    "            # hid = tanh(b + W * sent_embed)\n",
    "            # but it's faster to use affine_transform in dynet\n",
    "            hid = dy.affine_transform([b_hid, W_hid, sent_embed])\n",
    "            hid = dy.tanh(hid)\n",
    "\n",
    "            y_score = dy.affine_transform([b_clf, w_clf, hid])\n",
    "            y_proba = dy.logistic(y_score)\n",
    "            probas.append(y_proba)\n",
    "\n",
    "        return probas\n",
    "    \n",
    "    def update_best_parameters(self):\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "        \n",
    "    def set_best_parameters(self):\n",
    "        self.W_hid = self.best_W_hid\n",
    "        self.b_hid = self.best_b_hid \n",
    "\n",
    "        self.w_clf = self.best_w_clf\n",
    "        self.b_clf = self.best_b_clf\n",
    "        \n",
    "    def batch_loss(self, sents, train=True):\n",
    "        probas = self._predict(sents, train)\n",
    "\n",
    "        # we pack all predicted probas into one vector of length batch_size\n",
    "        probas = dy.concatenate(probas)\n",
    "\n",
    "        # we make a dynet vector out of the true ys\n",
    "        y_true = dy.inputVector([y for y, _ in sents])\n",
    "\n",
    "        # classification loss: we use the logistic loss\n",
    "        # this function automatically sums over all entries.\n",
    "        total_loss = dy.binary_log_loss(probas, y_true)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def num_correct(self, sents):\n",
    "        probas = self._predict(sents, train=False)\n",
    "        probas = [p.value() for p in probas]\n",
    "        y_true = [y for y, _ in sents]\n",
    "\n",
    "        correct = 0\n",
    "        # FIXME: count the number of correct predictions here\n",
    "        # Task 2\n",
    "        y_pred = []\n",
    "        for p in probas:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "\n",
    "        correct = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_true[i]])\n",
    "\n",
    "        return correct\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    clf = DANClassifier(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "    clf.embed.populate('embeds_baseline_lm', '/embed')\n",
    "\n",
    "    train_batches = make_batches(train_ix, BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, BATCH_SIZE)\n",
    "    \n",
    "    best_valid_acc = 0\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = clf.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate # correct pred.\n",
    "        valid_acc = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            valid_acc += clf.num_correct(batch)\n",
    "\n",
    "        valid_acc /= len(valid_ix)\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train loss: {:8.5f} \"\n",
    "               \"Valid accuracy: {:8.2f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            total_loss / len(train_ix),\n",
    "            valid_acc * 100\n",
    "            ))\n",
    "        \n",
    "        if valid_acc * 100 > best_valid_acc:\n",
    "            best_valid_acc = valid_acc * 100\n",
    "            print('Update parameters')\n",
    "            clf.update_best_parameters()\n",
    "    \n",
    "    with open(os.path.join('processed', 'test_ix.pkl'), 'rb') as f:\n",
    "        test_ix = pickle.load(f)\n",
    "    \n",
    "    test_batches = make_batches(test_ix, BATCH_SIZE)\n",
    "    # iterate over all test batches, accumulate # correct pred.\n",
    "    test_acc = 0\n",
    "    clf.set_best_parameters()\n",
    "    \n",
    "    for batch in test_batches:\n",
    "        dy.renew_cg()\n",
    "        test_acc += clf.num_correct(batch)\n",
    "\n",
    "    test_acc /= len(test_ix)\n",
    "    print((\"Running model on test set. Test accuracy: {:8.2f}\").format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 2.1s. Train loss:  0.69231 Valid accuracy:    56.08\n",
      "Update parameters\n",
      "Epoch   1 took 2.2s. Train loss:  0.68133 Valid accuracy:    57.48\n",
      "Update parameters\n",
      "Epoch   2 took 2.1s. Train loss:  0.67557 Valid accuracy:    58.16\n",
      "Update parameters\n",
      "Epoch   3 took 2.1s. Train loss:  0.66312 Valid accuracy:    59.08\n",
      "Update parameters\n",
      "Epoch   4 took 2.4s. Train loss:  0.65330 Valid accuracy:    59.67\n",
      "Update parameters\n",
      "Epoch   5 took 2.2s. Train loss:  0.64238 Valid accuracy:    59.98\n",
      "Update parameters\n",
      "Epoch   6 took 1.9s. Train loss:  0.63270 Valid accuracy:    60.27\n",
      "Update parameters\n",
      "Epoch   7 took 2.0s. Train loss:  0.61879 Valid accuracy:    60.68\n",
      "Update parameters\n",
      "Epoch   8 took 2.0s. Train loss:  0.60511 Valid accuracy:    60.46\n",
      "Epoch   9 took 1.9s. Train loss:  0.59230 Valid accuracy:    60.25\n",
      "Epoch  10 took 2.0s. Train loss:  0.57991 Valid accuracy:    60.39\n",
      "Epoch  11 took 2.1s. Train loss:  0.56963 Valid accuracy:    60.55\n",
      "Epoch  12 took 2.2s. Train loss:  0.55948 Valid accuracy:    60.81\n",
      "Update parameters\n",
      "Epoch  13 took 2.3s. Train loss:  0.55533 Valid accuracy:    60.97\n",
      "Update parameters\n",
      "Epoch  14 took 2.3s. Train loss:  0.54062 Valid accuracy:    60.91\n",
      "Epoch  15 took 2.2s. Train loss:  0.53409 Valid accuracy:    60.41\n",
      "Epoch  16 took 2.2s. Train loss:  0.52888 Valid accuracy:    60.77\n",
      "Epoch  17 took 2.1s. Train loss:  0.51909 Valid accuracy:    60.78\n",
      "Epoch  18 took 2.0s. Train loss:  0.50950 Valid accuracy:    60.54\n",
      "Epoch  19 took 2.2s. Train loss:  0.49854 Valid accuracy:    60.61\n",
      "Running model on test set. Test accuracy:    59.21\n"
     ]
    }
   ],
   "source": [
    "# Task 7 bigram embedding with unlabeled data\n",
    "\"\"\"Simple deep averaging net classifier\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class DANClassifier(object):\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "        \n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.w_clf = params.add_parameters((1, hidden_dim))\n",
    "        self.b_clf = params.add_parameters((1))\n",
    "        \n",
    "        # Parameters used for test set\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "\n",
    "\n",
    "    def _predict(self, batch, train=True):\n",
    "\n",
    "        # load the network parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "        w_clf = dy.parameter(self.w_clf)\n",
    "        b_clf = dy.parameter(self.b_clf)\n",
    "\n",
    "        probas = []\n",
    "        # predict the probability of positive sentiment for each sentence\n",
    "        for _, sent in batch:\n",
    "            sent_embed = [dy.lookup(self.embed, w) for w in sent]\n",
    "            # Task 3\n",
    "            if train == True:\n",
    "                for i in range(len(sent_embed)):\n",
    "                    sent_embed[i] = dy.dropout(sent_embed[i], 0.5)\n",
    "\n",
    "            sent_embed = dy.average(sent_embed)\n",
    "\n",
    "            # hid = tanh(b + W * sent_embed)\n",
    "            # but it's faster to use affine_transform in dynet\n",
    "            hid = dy.affine_transform([b_hid, W_hid, sent_embed])\n",
    "            hid = dy.tanh(hid)\n",
    "\n",
    "            y_score = dy.affine_transform([b_clf, w_clf, hid])\n",
    "            y_proba = dy.logistic(y_score)\n",
    "            probas.append(y_proba)\n",
    "\n",
    "        return probas\n",
    "    \n",
    "    def update_best_parameters(self):\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "        \n",
    "    def set_best_parameters(self):\n",
    "        self.W_hid = self.best_W_hid\n",
    "        self.b_hid = self.best_b_hid \n",
    "\n",
    "        self.w_clf = self.best_w_clf\n",
    "        self.b_clf = self.best_b_clf\n",
    "        \n",
    "    def batch_loss(self, sents, train=True):\n",
    "        probas = self._predict(sents, train)\n",
    "\n",
    "        # we pack all predicted probas into one vector of length batch_size\n",
    "        probas = dy.concatenate(probas)\n",
    "\n",
    "        # we make a dynet vector out of the true ys\n",
    "        y_true = dy.inputVector([y for y, _ in sents])\n",
    "\n",
    "        # classification loss: we use the logistic loss\n",
    "        # this function automatically sums over all entries.\n",
    "        total_loss = dy.binary_log_loss(probas, y_true)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def num_correct(self, sents):\n",
    "        probas = self._predict(sents, train=False)\n",
    "        probas = [p.value() for p in probas]\n",
    "        y_true = [y for y, _ in sents]\n",
    "\n",
    "        correct = 0\n",
    "        # FIXME: count the number of correct predictions here\n",
    "        # Task 2\n",
    "        y_pred = []\n",
    "        for p in probas:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "\n",
    "        correct = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_true[i]])\n",
    "\n",
    "        return correct\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    clf = DANClassifier(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "    clf.embed.populate('embeds_baseline_lm_unlabeled', '/embed')\n",
    "\n",
    "    train_batches = make_batches(train_ix, BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, BATCH_SIZE)\n",
    "    \n",
    "    best_valid_acc = 0\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = clf.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate # correct pred.\n",
    "        valid_acc = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            valid_acc += clf.num_correct(batch)\n",
    "\n",
    "        valid_acc /= len(valid_ix)\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train loss: {:8.5f} \"\n",
    "               \"Valid accuracy: {:8.2f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            total_loss / len(train_ix),\n",
    "            valid_acc * 100\n",
    "            ))\n",
    "        \n",
    "        if valid_acc * 100 > best_valid_acc:\n",
    "            best_valid_acc = valid_acc * 100\n",
    "            print('Update parameters')\n",
    "            clf.update_best_parameters()\n",
    "    \n",
    "    with open(os.path.join('processed', 'test_ix.pkl'), 'rb') as f:\n",
    "        test_ix = pickle.load(f)\n",
    "    \n",
    "    test_batches = make_batches(test_ix, BATCH_SIZE)\n",
    "    # iterate over all test batches, accumulate # correct pred.\n",
    "    test_acc = 0\n",
    "    clf.set_best_parameters()\n",
    "    \n",
    "    for batch in test_batches:\n",
    "        dy.renew_cg()\n",
    "        test_acc += clf.num_correct(batch)\n",
    "\n",
    "    test_acc /= len(test_ix)\n",
    "    print((\"Running model on test set. Test accuracy: {:8.2f}\").format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 1.9s. Train loss:  0.69132 Valid accuracy:    56.49\n",
      "Update parameters\n",
      "Epoch   1 took 2.3s. Train loss:  0.67760 Valid accuracy:    57.43\n",
      "Update parameters\n",
      "Epoch   2 took 2.2s. Train loss:  0.66591 Valid accuracy:    58.62\n",
      "Update parameters\n",
      "Epoch   3 took 1.9s. Train loss:  0.65026 Valid accuracy:    59.20\n",
      "Update parameters\n",
      "Epoch   4 took 2.3s. Train loss:  0.63536 Valid accuracy:    59.89\n",
      "Update parameters\n",
      "Epoch   5 took 1.9s. Train loss:  0.62346 Valid accuracy:    60.11\n",
      "Update parameters\n",
      "Epoch   6 took 2.0s. Train loss:  0.60871 Valid accuracy:    60.29\n",
      "Update parameters\n",
      "Epoch   7 took 2.2s. Train loss:  0.59724 Valid accuracy:    60.52\n",
      "Update parameters\n",
      "Epoch   8 took 2.3s. Train loss:  0.58564 Valid accuracy:    60.73\n",
      "Update parameters\n",
      "Epoch   9 took 1.8s. Train loss:  0.57325 Valid accuracy:    60.64\n",
      "Epoch  10 took 2.1s. Train loss:  0.56089 Valid accuracy:    60.67\n",
      "Epoch  11 took 2.2s. Train loss:  0.55013 Valid accuracy:    60.62\n",
      "Epoch  12 took 2.0s. Train loss:  0.54747 Valid accuracy:    60.77\n",
      "Update parameters\n",
      "Epoch  13 took 2.1s. Train loss:  0.53032 Valid accuracy:    60.59\n",
      "Epoch  14 took 2.2s. Train loss:  0.52327 Valid accuracy:    60.67\n",
      "Epoch  15 took 2.2s. Train loss:  0.51265 Valid accuracy:    60.63\n",
      "Epoch  16 took 2.2s. Train loss:  0.50492 Valid accuracy:    60.40\n",
      "Epoch  17 took 2.2s. Train loss:  0.50420 Valid accuracy:    60.35\n",
      "Epoch  18 took 2.0s. Train loss:  0.49325 Valid accuracy:    60.67\n",
      "Epoch  19 took 1.9s. Train loss:  0.48876 Valid accuracy:    60.37\n",
      "Running model on test set. Test accuracy:    59.47\n"
     ]
    }
   ],
   "source": [
    "# Task 7 trigram embedding\n",
    "\"\"\"Simple deep averaging net classifier\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class DANClassifier(object):\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "        \n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.w_clf = params.add_parameters((1, hidden_dim))\n",
    "        self.b_clf = params.add_parameters((1))\n",
    "        \n",
    "        # Parameters used for test set\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "\n",
    "\n",
    "    def _predict(self, batch, train=True):\n",
    "\n",
    "        # load the network parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "        w_clf = dy.parameter(self.w_clf)\n",
    "        b_clf = dy.parameter(self.b_clf)\n",
    "\n",
    "        probas = []\n",
    "        # predict the probability of positive sentiment for each sentence\n",
    "        for _, sent in batch:\n",
    "            sent_embed = [dy.lookup(self.embed, w) for w in sent]\n",
    "            # Task 3\n",
    "            if train == True:\n",
    "                for i in range(len(sent_embed)):\n",
    "                    sent_embed[i] = dy.dropout(sent_embed[i], 0.5)\n",
    "\n",
    "            sent_embed = dy.average(sent_embed)\n",
    "\n",
    "            # hid = tanh(b + W * sent_embed)\n",
    "            # but it's faster to use affine_transform in dynet\n",
    "            hid = dy.affine_transform([b_hid, W_hid, sent_embed])\n",
    "            hid = dy.tanh(hid)\n",
    "\n",
    "            y_score = dy.affine_transform([b_clf, w_clf, hid])\n",
    "            y_proba = dy.logistic(y_score)\n",
    "            probas.append(y_proba)\n",
    "\n",
    "        return probas\n",
    "    \n",
    "    def update_best_parameters(self):\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "        \n",
    "    def set_best_parameters(self):\n",
    "        self.W_hid = self.best_W_hid\n",
    "        self.b_hid = self.best_b_hid \n",
    "\n",
    "        self.w_clf = self.best_w_clf\n",
    "        self.b_clf = self.best_b_clf\n",
    "        \n",
    "    def batch_loss(self, sents, train=True):\n",
    "        probas = self._predict(sents, train)\n",
    "\n",
    "        # we pack all predicted probas into one vector of length batch_size\n",
    "        probas = dy.concatenate(probas)\n",
    "\n",
    "        # we make a dynet vector out of the true ys\n",
    "        y_true = dy.inputVector([y for y, _ in sents])\n",
    "\n",
    "        # classification loss: we use the logistic loss\n",
    "        # this function automatically sums over all entries.\n",
    "        total_loss = dy.binary_log_loss(probas, y_true)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def num_correct(self, sents):\n",
    "        probas = self._predict(sents, train=False)\n",
    "        probas = [p.value() for p in probas]\n",
    "        y_true = [y for y, _ in sents]\n",
    "\n",
    "        correct = 0\n",
    "        # FIXME: count the number of correct predictions here\n",
    "        # Task 2\n",
    "        y_pred = []\n",
    "        for p in probas:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "\n",
    "        correct = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_true[i]])\n",
    "\n",
    "        return correct\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    clf = DANClassifier(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "    clf.embed.populate('embeds_baseline_lm_trigram', '/embed')\n",
    "\n",
    "    train_batches = make_batches(train_ix, BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, BATCH_SIZE)\n",
    "    \n",
    "    best_valid_acc = 0\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = clf.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate # correct pred.\n",
    "        valid_acc = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            valid_acc += clf.num_correct(batch)\n",
    "\n",
    "        valid_acc /= len(valid_ix)\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train loss: {:8.5f} \"\n",
    "               \"Valid accuracy: {:8.2f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            total_loss / len(train_ix),\n",
    "            valid_acc * 100\n",
    "            ))\n",
    "        \n",
    "        if valid_acc * 100 > best_valid_acc:\n",
    "            best_valid_acc = valid_acc * 100\n",
    "            print('Update parameters')\n",
    "            clf.update_best_parameters()\n",
    "    \n",
    "    with open(os.path.join('processed', 'test_ix.pkl'), 'rb') as f:\n",
    "        test_ix = pickle.load(f)\n",
    "    \n",
    "    test_batches = make_batches(test_ix, BATCH_SIZE)\n",
    "    # iterate over all test batches, accumulate # correct pred.\n",
    "    test_acc = 0\n",
    "    clf.set_best_parameters()\n",
    "    \n",
    "    for batch in test_batches:\n",
    "        dy.renew_cg()\n",
    "        test_acc += clf.num_correct(batch)\n",
    "\n",
    "    test_acc /= len(test_ix)\n",
    "    print((\"Running model on test set. Test accuracy: {:8.2f}\").format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 2.2s. Train loss:  0.69534 Valid accuracy:    56.86\n",
      "Update parameters\n",
      "Epoch   1 took 2.2s. Train loss:  0.67763 Valid accuracy:    57.78\n",
      "Update parameters\n",
      "Epoch   2 took 2.0s. Train loss:  0.66758 Valid accuracy:    58.72\n",
      "Update parameters\n",
      "Epoch   3 took 2.0s. Train loss:  0.65662 Valid accuracy:    59.58\n",
      "Update parameters\n",
      "Epoch   4 took 2.2s. Train loss:  0.64254 Valid accuracy:    59.70\n",
      "Update parameters\n",
      "Epoch   5 took 2.1s. Train loss:  0.62767 Valid accuracy:    60.23\n",
      "Update parameters\n",
      "Epoch   6 took 2.1s. Train loss:  0.61622 Valid accuracy:    60.48\n",
      "Update parameters\n",
      "Epoch   7 took 2.3s. Train loss:  0.59955 Valid accuracy:    60.46\n",
      "Epoch   8 took 2.2s. Train loss:  0.59491 Valid accuracy:    60.56\n",
      "Update parameters\n",
      "Epoch   9 took 1.9s. Train loss:  0.57715 Valid accuracy:    60.74\n",
      "Update parameters\n",
      "Epoch  10 took 2.3s. Train loss:  0.56597 Valid accuracy:    60.77\n",
      "Update parameters\n",
      "Epoch  11 took 2.2s. Train loss:  0.55669 Valid accuracy:    60.84\n",
      "Update parameters\n",
      "Epoch  12 took 2.1s. Train loss:  0.54709 Valid accuracy:    60.58\n",
      "Epoch  13 took 2.1s. Train loss:  0.53395 Valid accuracy:    60.67\n",
      "Epoch  14 took 2.2s. Train loss:  0.53398 Valid accuracy:    60.77\n",
      "Epoch  15 took 2.0s. Train loss:  0.52180 Valid accuracy:    60.45\n",
      "Epoch  16 took 2.2s. Train loss:  0.50683 Valid accuracy:    60.82\n",
      "Epoch  17 took 2.2s. Train loss:  0.50064 Valid accuracy:    61.15\n",
      "Update parameters\n",
      "Epoch  18 took 2.0s. Train loss:  0.50486 Valid accuracy:    60.97\n",
      "Epoch  19 took 2.1s. Train loss:  0.49189 Valid accuracy:    60.36\n",
      "Running model on test set. Test accuracy:    59.44\n"
     ]
    }
   ],
   "source": [
    "# Task 7 trigram embedding with unlabeled data (best embeddings by validation set?)\n",
    "\"\"\"Simple deep averaging net classifier\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class DANClassifier(object):\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "        \n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.w_clf = params.add_parameters((1, hidden_dim))\n",
    "        self.b_clf = params.add_parameters((1))\n",
    "        \n",
    "        # Parameters used for test set\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "\n",
    "\n",
    "    def _predict(self, batch, train=True):\n",
    "\n",
    "        # load the network parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "        w_clf = dy.parameter(self.w_clf)\n",
    "        b_clf = dy.parameter(self.b_clf)\n",
    "\n",
    "        probas = []\n",
    "        # predict the probability of positive sentiment for each sentence\n",
    "        for _, sent in batch:\n",
    "            sent_embed = [dy.lookup(self.embed, w) for w in sent]\n",
    "            # Task 3\n",
    "            if train == True:\n",
    "                for i in range(len(sent_embed)):\n",
    "                    sent_embed[i] = dy.dropout(sent_embed[i], 0.5)\n",
    "\n",
    "            sent_embed = dy.average(sent_embed)\n",
    "\n",
    "            # hid = tanh(b + W * sent_embed)\n",
    "            # but it's faster to use affine_transform in dynet\n",
    "            hid = dy.affine_transform([b_hid, W_hid, sent_embed])\n",
    "            hid = dy.tanh(hid)\n",
    "\n",
    "            y_score = dy.affine_transform([b_clf, w_clf, hid])\n",
    "            y_proba = dy.logistic(y_score)\n",
    "            probas.append(y_proba)\n",
    "\n",
    "        return probas\n",
    "    \n",
    "    def update_best_parameters(self):\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "        \n",
    "    def set_best_parameters(self):\n",
    "        self.W_hid = self.best_W_hid\n",
    "        self.b_hid = self.best_b_hid \n",
    "\n",
    "        self.w_clf = self.best_w_clf\n",
    "        self.b_clf = self.best_b_clf\n",
    "        \n",
    "    def batch_loss(self, sents, train=True):\n",
    "        probas = self._predict(sents, train)\n",
    "\n",
    "        # we pack all predicted probas into one vector of length batch_size\n",
    "        probas = dy.concatenate(probas)\n",
    "\n",
    "        # we make a dynet vector out of the true ys\n",
    "        y_true = dy.inputVector([y for y, _ in sents])\n",
    "\n",
    "        # classification loss: we use the logistic loss\n",
    "        # this function automatically sums over all entries.\n",
    "        total_loss = dy.binary_log_loss(probas, y_true)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def num_correct(self, sents):\n",
    "        probas = self._predict(sents, train=False)\n",
    "        probas = [p.value() for p in probas]\n",
    "        y_true = [y for y, _ in sents]\n",
    "\n",
    "        correct = 0\n",
    "        # FIXME: count the number of correct predictions here\n",
    "        # Task 2\n",
    "        y_pred = []\n",
    "        for p in probas:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "\n",
    "        correct = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_true[i]])\n",
    "\n",
    "        return correct\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    clf = DANClassifier(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "    clf.embed.populate('embeds_baseline_lm_trigram_unlabeled', '/embed')\n",
    "\n",
    "    train_batches = make_batches(train_ix, BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, BATCH_SIZE)\n",
    "    \n",
    "    best_valid_acc = 0\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = clf.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate # correct pred.\n",
    "        valid_acc = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            valid_acc += clf.num_correct(batch)\n",
    "\n",
    "        valid_acc /= len(valid_ix)\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train loss: {:8.5f} \"\n",
    "               \"Valid accuracy: {:8.2f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            total_loss / len(train_ix),\n",
    "            valid_acc * 100\n",
    "            ))\n",
    "        \n",
    "        if valid_acc * 100 > best_valid_acc:\n",
    "            best_valid_acc = valid_acc * 100\n",
    "            print('Update parameters')\n",
    "            clf.update_best_parameters()\n",
    "    \n",
    "    with open(os.path.join('processed', 'test_ix.pkl'), 'rb') as f:\n",
    "        test_ix = pickle.load(f)\n",
    "    \n",
    "    test_batches = make_batches(test_ix, BATCH_SIZE)\n",
    "    # iterate over all test batches, accumulate # correct pred.\n",
    "    test_acc = 0\n",
    "    clf.set_best_parameters()\n",
    "    \n",
    "    for batch in test_batches:\n",
    "        dy.renew_cg()\n",
    "        test_acc += clf.num_correct(batch)\n",
    "\n",
    "    test_acc /= len(test_ix)\n",
    "    print((\"Running model on test set. Test accuracy: {:8.2f}\").format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 2.0s. Train loss:  0.69070 Valid accuracy:    56.67\n",
      "Update parameters\n",
      "Epoch   1 took 2.1s. Train loss:  0.67834 Valid accuracy:    57.98\n",
      "Update parameters\n",
      "Epoch   2 took 2.2s. Train loss:  0.66457 Valid accuracy:    59.01\n",
      "Update parameters\n",
      "Epoch   3 took 2.2s. Train loss:  0.64902 Valid accuracy:    59.75\n",
      "Update parameters\n",
      "Epoch   4 took 2.2s. Train loss:  0.63394 Valid accuracy:    59.83\n",
      "Update parameters\n",
      "Epoch   5 took 2.2s. Train loss:  0.62064 Valid accuracy:    60.17\n",
      "Update parameters\n",
      "Epoch   6 took 2.2s. Train loss:  0.60835 Valid accuracy:    60.29\n",
      "Update parameters\n",
      "Epoch   7 took 2.1s. Train loss:  0.59309 Valid accuracy:    60.36\n",
      "Update parameters\n",
      "Epoch   8 took 2.0s. Train loss:  0.57988 Valid accuracy:    60.56\n",
      "Update parameters\n",
      "Epoch   9 took 2.1s. Train loss:  0.56776 Valid accuracy:    60.76\n",
      "Update parameters\n",
      "Epoch  10 took 2.1s. Train loss:  0.55464 Valid accuracy:    60.71\n",
      "Epoch  11 took 1.8s. Train loss:  0.54639 Valid accuracy:    60.83\n",
      "Update parameters\n",
      "Epoch  12 took 2.0s. Train loss:  0.54046 Valid accuracy:    60.83\n",
      "Update parameters\n",
      "Epoch  13 took 2.0s. Train loss:  0.52741 Valid accuracy:    60.77\n",
      "Epoch  14 took 2.0s. Train loss:  0.51725 Valid accuracy:    60.91\n",
      "Update parameters\n",
      "Epoch  15 took 2.2s. Train loss:  0.51269 Valid accuracy:    60.38\n",
      "Epoch  16 took 2.3s. Train loss:  0.50838 Valid accuracy:    60.55\n",
      "Epoch  17 took 2.3s. Train loss:  0.49844 Valid accuracy:    60.49\n",
      "Epoch  18 took 2.0s. Train loss:  0.49462 Valid accuracy:    60.49\n",
      "Epoch  19 took 1.9s. Train loss:  0.48923 Valid accuracy:    60.21\n",
      "Running model on test set. Test accuracy:    59.29\n"
     ]
    }
   ],
   "source": [
    "# Task 7 4-gram embedding\n",
    "\"\"\"Simple deep averaging net classifier\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class DANClassifier(object):\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "        \n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.w_clf = params.add_parameters((1, hidden_dim))\n",
    "        self.b_clf = params.add_parameters((1))\n",
    "        \n",
    "        # Parameters used for test set\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "\n",
    "\n",
    "    def _predict(self, batch, train=True):\n",
    "\n",
    "        # load the network parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "        w_clf = dy.parameter(self.w_clf)\n",
    "        b_clf = dy.parameter(self.b_clf)\n",
    "\n",
    "        probas = []\n",
    "        # predict the probability of positive sentiment for each sentence\n",
    "        for _, sent in batch:\n",
    "            sent_embed = [dy.lookup(self.embed, w) for w in sent]\n",
    "            # Task 3\n",
    "            if train == True:\n",
    "                for i in range(len(sent_embed)):\n",
    "                    sent_embed[i] = dy.dropout(sent_embed[i], 0.5)\n",
    "\n",
    "            sent_embed = dy.average(sent_embed)\n",
    "\n",
    "            # hid = tanh(b + W * sent_embed)\n",
    "            # but it's faster to use affine_transform in dynet\n",
    "            hid = dy.affine_transform([b_hid, W_hid, sent_embed])\n",
    "            hid = dy.tanh(hid)\n",
    "\n",
    "            y_score = dy.affine_transform([b_clf, w_clf, hid])\n",
    "            y_proba = dy.logistic(y_score)\n",
    "            probas.append(y_proba)\n",
    "\n",
    "        return probas\n",
    "    \n",
    "    def update_best_parameters(self):\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "        \n",
    "    def set_best_parameters(self):\n",
    "        self.W_hid = self.best_W_hid\n",
    "        self.b_hid = self.best_b_hid \n",
    "\n",
    "        self.w_clf = self.best_w_clf\n",
    "        self.b_clf = self.best_b_clf\n",
    "        \n",
    "    def batch_loss(self, sents, train=True):\n",
    "        probas = self._predict(sents, train)\n",
    "\n",
    "        # we pack all predicted probas into one vector of length batch_size\n",
    "        probas = dy.concatenate(probas)\n",
    "\n",
    "        # we make a dynet vector out of the true ys\n",
    "        y_true = dy.inputVector([y for y, _ in sents])\n",
    "\n",
    "        # classification loss: we use the logistic loss\n",
    "        # this function automatically sums over all entries.\n",
    "        total_loss = dy.binary_log_loss(probas, y_true)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def num_correct(self, sents):\n",
    "        probas = self._predict(sents, train=False)\n",
    "        probas = [p.value() for p in probas]\n",
    "        y_true = [y for y, _ in sents]\n",
    "\n",
    "        correct = 0\n",
    "        # FIXME: count the number of correct predictions here\n",
    "        # Task 2\n",
    "        y_pred = []\n",
    "        for p in probas:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "\n",
    "        correct = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_true[i]])\n",
    "\n",
    "        return correct\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    clf = DANClassifier(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "    clf.embed.populate('embeds_baseline_lm_4gram', '/embed')\n",
    "\n",
    "    train_batches = make_batches(train_ix, BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, BATCH_SIZE)\n",
    "    \n",
    "    best_valid_acc = 0\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = clf.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate # correct pred.\n",
    "        valid_acc = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            valid_acc += clf.num_correct(batch)\n",
    "\n",
    "        valid_acc /= len(valid_ix)\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train loss: {:8.5f} \"\n",
    "               \"Valid accuracy: {:8.2f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            total_loss / len(train_ix),\n",
    "            valid_acc * 100\n",
    "            ))\n",
    "        \n",
    "        if valid_acc * 100 > best_valid_acc:\n",
    "            best_valid_acc = valid_acc * 100\n",
    "            print('Update parameters')\n",
    "            clf.update_best_parameters()\n",
    "    \n",
    "    with open(os.path.join('processed', 'test_ix.pkl'), 'rb') as f:\n",
    "        test_ix = pickle.load(f)\n",
    "    \n",
    "    test_batches = make_batches(test_ix, BATCH_SIZE)\n",
    "    # iterate over all test batches, accumulate # correct pred.\n",
    "    test_acc = 0\n",
    "    clf.set_best_parameters()\n",
    "    \n",
    "    for batch in test_batches:\n",
    "        dy.renew_cg()\n",
    "        test_acc += clf.num_correct(batch)\n",
    "\n",
    "    test_acc /= len(test_ix)\n",
    "    print((\"Running model on test set. Test accuracy: {:8.2f}\").format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 1.8s. Train loss:  0.68704 Valid accuracy:    56.91\n",
      "Update parameters\n",
      "Epoch   1 took 1.8s. Train loss:  0.67480 Valid accuracy:    58.25\n",
      "Update parameters\n",
      "Epoch   2 took 1.7s. Train loss:  0.66387 Valid accuracy:    59.11\n",
      "Update parameters\n",
      "Epoch   3 took 2.0s. Train loss:  0.64975 Valid accuracy:    59.67\n",
      "Update parameters\n",
      "Epoch   4 took 2.5s. Train loss:  0.63449 Valid accuracy:    60.07\n",
      "Update parameters\n",
      "Epoch   5 took 2.1s. Train loss:  0.61806 Valid accuracy:    60.72\n",
      "Update parameters\n",
      "Epoch   6 took 2.1s. Train loss:  0.60632 Valid accuracy:    60.68\n",
      "Epoch   7 took 2.2s. Train loss:  0.59015 Valid accuracy:    60.53\n",
      "Epoch   8 took 2.3s. Train loss:  0.58294 Valid accuracy:    60.75\n",
      "Update parameters\n",
      "Epoch   9 took 2.1s. Train loss:  0.56880 Valid accuracy:    60.72\n",
      "Epoch  10 took 2.4s. Train loss:  0.56126 Valid accuracy:    60.94\n",
      "Update parameters\n",
      "Epoch  11 took 2.2s. Train loss:  0.55378 Valid accuracy:    60.64\n",
      "Epoch  12 took 2.1s. Train loss:  0.54669 Valid accuracy:    60.79\n",
      "Epoch  13 took 2.2s. Train loss:  0.53382 Valid accuracy:    60.79\n",
      "Epoch  14 took 1.9s. Train loss:  0.52633 Valid accuracy:    61.10\n",
      "Update parameters\n",
      "Epoch  15 took 2.0s. Train loss:  0.51361 Valid accuracy:    60.77\n",
      "Epoch  16 took 2.2s. Train loss:  0.50521 Valid accuracy:    60.74\n",
      "Epoch  17 took 2.2s. Train loss:  0.50291 Valid accuracy:    60.87\n",
      "Epoch  18 took 2.3s. Train loss:  0.49408 Valid accuracy:    60.54\n",
      "Epoch  19 took 2.3s. Train loss:  0.48522 Valid accuracy:    60.61\n",
      "Running model on test set. Test accuracy:    59.86\n"
     ]
    }
   ],
   "source": [
    "# Task 7 4-gram embedding with unlabeled data (This result is also good)\n",
    "\"\"\"Simple deep averaging net classifier\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class DANClassifier(object):\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "        \n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.w_clf = params.add_parameters((1, hidden_dim))\n",
    "        self.b_clf = params.add_parameters((1))\n",
    "        \n",
    "        # Parameters used for test set\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "\n",
    "\n",
    "    def _predict(self, batch, train=True):\n",
    "\n",
    "        # load the network parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "        w_clf = dy.parameter(self.w_clf)\n",
    "        b_clf = dy.parameter(self.b_clf)\n",
    "\n",
    "        probas = []\n",
    "        # predict the probability of positive sentiment for each sentence\n",
    "        for _, sent in batch:\n",
    "            sent_embed = [dy.lookup(self.embed, w) for w in sent]\n",
    "            # Task 3\n",
    "            if train == True:\n",
    "                for i in range(len(sent_embed)):\n",
    "                    sent_embed[i] = dy.dropout(sent_embed[i], 0.5)\n",
    "\n",
    "            sent_embed = dy.average(sent_embed)\n",
    "\n",
    "            # hid = tanh(b + W * sent_embed)\n",
    "            # but it's faster to use affine_transform in dynet\n",
    "            hid = dy.affine_transform([b_hid, W_hid, sent_embed])\n",
    "            hid = dy.tanh(hid)\n",
    "\n",
    "            y_score = dy.affine_transform([b_clf, w_clf, hid])\n",
    "            y_proba = dy.logistic(y_score)\n",
    "            probas.append(y_proba)\n",
    "\n",
    "        return probas\n",
    "    \n",
    "    def update_best_parameters(self):\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "        \n",
    "    def set_best_parameters(self):\n",
    "        self.W_hid = self.best_W_hid\n",
    "        self.b_hid = self.best_b_hid \n",
    "\n",
    "        self.w_clf = self.best_w_clf\n",
    "        self.b_clf = self.best_b_clf\n",
    "        \n",
    "    def batch_loss(self, sents, train=True):\n",
    "        probas = self._predict(sents, train)\n",
    "\n",
    "        # we pack all predicted probas into one vector of length batch_size\n",
    "        probas = dy.concatenate(probas)\n",
    "\n",
    "        # we make a dynet vector out of the true ys\n",
    "        y_true = dy.inputVector([y for y, _ in sents])\n",
    "\n",
    "        # classification loss: we use the logistic loss\n",
    "        # this function automatically sums over all entries.\n",
    "        total_loss = dy.binary_log_loss(probas, y_true)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def num_correct(self, sents):\n",
    "        probas = self._predict(sents, train=False)\n",
    "        probas = [p.value() for p in probas]\n",
    "        y_true = [y for y, _ in sents]\n",
    "\n",
    "        correct = 0\n",
    "        # FIXME: count the number of correct predictions here\n",
    "        # Task 2\n",
    "        y_pred = []\n",
    "        for p in probas:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "\n",
    "        correct = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_true[i]])\n",
    "\n",
    "        return correct\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    clf = DANClassifier(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "    clf.embed.populate('embeds_baseline_lm_4gram_unlabeled', '/embed')\n",
    "\n",
    "    train_batches = make_batches(train_ix, BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, BATCH_SIZE)\n",
    "    \n",
    "    best_valid_acc = 0\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = clf.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate # correct pred.\n",
    "        valid_acc = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            valid_acc += clf.num_correct(batch)\n",
    "\n",
    "        valid_acc /= len(valid_ix)\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train loss: {:8.5f} \"\n",
    "               \"Valid accuracy: {:8.2f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            total_loss / len(train_ix),\n",
    "            valid_acc * 100\n",
    "            ))\n",
    "        \n",
    "        if valid_acc * 100 > best_valid_acc:\n",
    "            best_valid_acc = valid_acc * 100\n",
    "            print('Update parameters')\n",
    "            clf.update_best_parameters()\n",
    "    \n",
    "    with open(os.path.join('processed', 'test_ix.pkl'), 'rb') as f:\n",
    "        test_ix = pickle.load(f)\n",
    "    \n",
    "    test_batches = make_batches(test_ix, BATCH_SIZE)\n",
    "    # iterate over all test batches, accumulate # correct pred.\n",
    "    test_acc = 0\n",
    "    clf.set_best_parameters()\n",
    "    \n",
    "    for batch in test_batches:\n",
    "        dy.renew_cg()\n",
    "        test_acc += clf.num_correct(batch)\n",
    "\n",
    "    test_acc /= len(test_ix)\n",
    "    print((\"Running model on test set. Test accuracy: {:8.2f}\").format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 took 2.3s. Train loss:  0.69129 Valid accuracy:    56.78\n",
      "Update parameters\n",
      "Epoch   1 took 2.1s. Train loss:  0.66809 Valid accuracy:    58.61\n",
      "Update parameters\n",
      "Epoch   2 took 2.3s. Train loss:  0.64600 Valid accuracy:    59.61\n",
      "Update parameters\n",
      "Epoch   3 took 2.2s. Train loss:  0.62200 Valid accuracy:    60.09\n",
      "Update parameters\n",
      "Epoch   4 took 2.4s. Train loss:  0.60083 Valid accuracy:    60.38\n",
      "Update parameters\n",
      "Epoch   5 took 2.2s. Train loss:  0.58465 Valid accuracy:    60.39\n",
      "Update parameters\n",
      "Epoch   6 took 2.1s. Train loss:  0.56873 Valid accuracy:    60.36\n",
      "Epoch   7 took 2.3s. Train loss:  0.55274 Valid accuracy:    60.10\n",
      "Epoch   8 took 2.0s. Train loss:  0.54387 Valid accuracy:    60.57\n",
      "Update parameters\n",
      "Epoch   9 took 2.1s. Train loss:  0.53253 Valid accuracy:    60.59\n",
      "Update parameters\n",
      "Epoch  10 took 2.3s. Train loss:  0.51981 Valid accuracy:    60.45\n",
      "Epoch  11 took 2.2s. Train loss:  0.50746 Valid accuracy:    60.22\n",
      "Epoch  12 took 2.0s. Train loss:  0.49908 Valid accuracy:    60.33\n",
      "Epoch  13 took 2.2s. Train loss:  0.49432 Valid accuracy:    60.14\n",
      "Epoch  14 took 2.2s. Train loss:  0.49186 Valid accuracy:    60.40\n",
      "Epoch  15 took 1.8s. Train loss:  0.48756 Valid accuracy:    60.23\n",
      "Epoch  16 took 2.2s. Train loss:  0.47320 Valid accuracy:    59.97\n",
      "Epoch  17 took 2.3s. Train loss:  0.47220 Valid accuracy:    60.13\n",
      "Epoch  18 took 2.1s. Train loss:  0.46528 Valid accuracy:    60.29\n",
      "Epoch  19 took 2.3s. Train loss:  0.45970 Valid accuracy:    60.23\n",
      "Running model on test set. Test accuracy:    59.25\n"
     ]
    }
   ],
   "source": [
    "# Task 7 without preinitialized embeddings\n",
    "\"\"\"Simple deep averaging net classifier\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from time import clock\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=42, autobatch=1)\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(vocab)#__FIXME__\n",
    "\n",
    "\n",
    "def make_batches(data, batch_size):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for pair in data:\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "        batch.append(pair)\n",
    "\n",
    "    if batch:\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class DANClassifier(object):\n",
    "    def __init__(self, params, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = params.add_lookup_parameters((vocab_size, hidden_dim))\n",
    "        \n",
    "        self.W_hid = params.add_parameters((hidden_dim, hidden_dim))\n",
    "        self.b_hid = params.add_parameters((hidden_dim))\n",
    "\n",
    "        self.w_clf = params.add_parameters((1, hidden_dim))\n",
    "        self.b_clf = params.add_parameters((1))\n",
    "        \n",
    "        # Parameters used for test set\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "\n",
    "\n",
    "    def _predict(self, batch, train=True):\n",
    "\n",
    "        # load the network parameters\n",
    "        W_hid = dy.parameter(self.W_hid)\n",
    "        b_hid = dy.parameter(self.b_hid)\n",
    "        w_clf = dy.parameter(self.w_clf)\n",
    "        b_clf = dy.parameter(self.b_clf)\n",
    "\n",
    "        probas = []\n",
    "        # predict the probability of positive sentiment for each sentence\n",
    "        for _, sent in batch:\n",
    "            sent_embed = [dy.lookup(self.embed, w) for w in sent]\n",
    "            # Task 3\n",
    "            if train == True:\n",
    "                for i in range(len(sent_embed)):\n",
    "                    sent_embed[i] = dy.dropout(sent_embed[i], 0.5)\n",
    "\n",
    "            sent_embed = dy.average(sent_embed)\n",
    "\n",
    "            # hid = tanh(b + W * sent_embed)\n",
    "            # but it's faster to use affine_transform in dynet\n",
    "            hid = dy.affine_transform([b_hid, W_hid, sent_embed])\n",
    "            hid = dy.tanh(hid)\n",
    "\n",
    "            y_score = dy.affine_transform([b_clf, w_clf, hid])\n",
    "            y_proba = dy.logistic(y_score)\n",
    "            probas.append(y_proba)\n",
    "\n",
    "        return probas\n",
    "    \n",
    "    def update_best_parameters(self):\n",
    "        self.best_W_hid = self.W_hid\n",
    "        self.best_b_hid = self.b_hid\n",
    "\n",
    "        self.best_w_clf = self.w_clf\n",
    "        self.best_b_clf = self.b_clf\n",
    "        \n",
    "    def set_best_parameters(self):\n",
    "        self.W_hid = self.best_W_hid\n",
    "        self.b_hid = self.best_b_hid \n",
    "\n",
    "        self.w_clf = self.best_w_clf\n",
    "        self.b_clf = self.best_b_clf\n",
    "        \n",
    "    def batch_loss(self, sents, train=True):\n",
    "        probas = self._predict(sents, train)\n",
    "\n",
    "        # we pack all predicted probas into one vector of length batch_size\n",
    "        probas = dy.concatenate(probas)\n",
    "\n",
    "        # we make a dynet vector out of the true ys\n",
    "        y_true = dy.inputVector([y for y, _ in sents])\n",
    "\n",
    "        # classification loss: we use the logistic loss\n",
    "        # this function automatically sums over all entries.\n",
    "        total_loss = dy.binary_log_loss(probas, y_true)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def num_correct(self, sents):\n",
    "        probas = self._predict(sents, train=False)\n",
    "        probas = [p.value() for p in probas]\n",
    "        y_true = [y for y, _ in sents]\n",
    "\n",
    "        correct = 0\n",
    "        # FIXME: count the number of correct predictions here\n",
    "        # Task 2\n",
    "        y_pred = []\n",
    "        for p in probas:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(True)\n",
    "            else:\n",
    "                y_pred.append(False)\n",
    "\n",
    "        correct = sum([1 for i in range(len(y_pred)) if y_pred[i] == y_true[i]])\n",
    "\n",
    "        return correct\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(os.path.join('processed', 'train_ix.pkl'), 'rb') as f:\n",
    "        train_ix = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join('processed', 'valid_ix.pkl'), 'rb') as f:\n",
    "        valid_ix = pickle.load(f)\n",
    "\n",
    "    # initialize dynet parameters and learning algorithm\n",
    "    params = dy.ParameterCollection()\n",
    "    trainer = dy.AdadeltaTrainer(params)\n",
    "    clf = DANClassifier(params, vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    train_batches = make_batches(train_ix, BATCH_SIZE)\n",
    "    valid_batches = make_batches(valid_ix, BATCH_SIZE)\n",
    "    \n",
    "    best_valid_acc = 0\n",
    "    for it in range(MAX_EPOCHS):\n",
    "        tic = clock()\n",
    "\n",
    "        # iterate over all training batches, accumulate loss.\n",
    "        total_loss = 0\n",
    "        for batch in train_batches:\n",
    "            dy.renew_cg()\n",
    "            loss = clf.batch_loss(batch, train=True)\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "            total_loss += loss.value()\n",
    "\n",
    "        # iterate over all validation batches, accumulate # correct pred.\n",
    "        valid_acc = 0\n",
    "        for batch in valid_batches:\n",
    "            dy.renew_cg()\n",
    "            valid_acc += clf.num_correct(batch)\n",
    "\n",
    "        valid_acc /= len(valid_ix)\n",
    "\n",
    "        toc = clock()\n",
    "\n",
    "        print((\"Epoch {:3d} took {:3.1f}s. \"\n",
    "               \"Train loss: {:8.5f} \"\n",
    "               \"Valid accuracy: {:8.2f}\").format(\n",
    "            it,\n",
    "            toc - tic,\n",
    "            total_loss / len(train_ix),\n",
    "            valid_acc * 100\n",
    "            ))\n",
    "        \n",
    "        if valid_acc * 100 > best_valid_acc:\n",
    "            best_valid_acc = valid_acc * 100\n",
    "            print('Update parameters')\n",
    "            clf.update_best_parameters()\n",
    "    \n",
    "    with open(os.path.join('processed', 'test_ix.pkl'), 'rb') as f:\n",
    "        test_ix = pickle.load(f)\n",
    "    \n",
    "    test_batches = make_batches(test_ix, BATCH_SIZE)\n",
    "    # iterate over all test batches, accumulate # correct pred.\n",
    "    test_acc = 0\n",
    "    clf.set_best_parameters()\n",
    "    \n",
    "    for batch in test_batches:\n",
    "        dy.renew_cg()\n",
    "        test_acc += clf.num_correct(batch)\n",
    "\n",
    "    test_acc /= len(test_ix)\n",
    "    print((\"Running model on test set. Test accuracy: {:8.2f}\").format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
